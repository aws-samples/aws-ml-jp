{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d73d16-fce1-4340-8857-9a7815f0773f",
   "metadata": {},
   "source": [
    "# æ—¥æœ¬èª BERT Base Model Fine-tuning & Deployment on Inferentia2/Trainium\n",
    "æœ¬ Notebook ã®å…ƒãƒã‚¿ã®ãƒ–ãƒ­ã‚°ã¯ã“ã¡ã‚‰ã‹ã‚‰\n",
    "+ https://aws.amazon.com/jp/blogs/news/aws-trainium-amazon-ec2-trn1-ml-training-part1/\n",
    "+ https://aws.amazon.com/jp/blogs/news/aws-trainium-amazon-ec2-trn1-ml-training-part2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0641e6b-67e8-4f10-86ef-1ce00c20162b",
   "metadata": {},
   "source": [
    "## äº‹å‰æº–å‚™\n",
    "æœ¬ notebookã¯ Neuron 2.14.0 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸ Amazon EC2 inf2.xlarge ä¸Šã§å‹•ä½œç¢ºèªã—ã¦ã„ã¾ã™ã€‚\n",
    "ï¼ˆã‚ˆã‚Šå¤§ãã„ã‚µã‚¤ã‚ºã® Inf2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŠã³ Trn1 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã§ã‚‚å®Ÿè¡Œå¯èƒ½ã§ã™ã€‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf706fb-f236-4b87-870f-4e7f81806dcc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U transformers[ja]==4.27.4 datasets\n",
    "!pip uninstall -y wandb # if installed, needs to setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa82ee-39d2-4e6c-9294-b920a1284890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip list | grep \"neuron\\|torch\\|transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23e3b2-6c97-411a-b0f0-e324b9f0f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dpkg --list | grep neuron\n",
    "# For Ubuntu Environment. Please use \"yum list installed\" for Amazon Linux Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2168c77-6ac7-4f71-95c7-06e6df4386fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rmmod neuron; sudo modprobe neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87668961-d047-4603-a3b1-a05544713952",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "æœ¬ãƒ†ã‚¹ãƒˆã§ã¯ã€Huggingface Hub ã§åˆ©ç”¨å¯èƒ½ãªä»¥ä¸‹ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆï¼ˆæ„Ÿæƒ…ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ã€æ—¥æœ¬èªã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "https://huggingface.co/datasets/tyqiangz/multilingual-sentiments\n",
    "\n",
    "æœ¬ãƒ†ã‚¹ãƒˆã§ã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ Positive ã‹ Negative ã«åˆ†é¡ã™ã‚‹ 2 ã‚¯ãƒ©ã‚¹ã®åˆ†é¡å•é¡Œã¨ã—ã¦æ‰±ã†ã“ã¨ã«ã—ã¾ã™ã€‚å…ƒã€…ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ `positive(LABEL_0)`ã€`neutral(LABEL_1)`ã€`negative(LABEL_2)`ã¨ã—ã¦ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚Œã¦ã„ã¾ã™ãŒã€neutral ã®ãƒ‡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã—ãªã„ã“ã¨ã¨ã—ã€ãƒ©ãƒ™ãƒ«ã‚’`positive(LABEL_0)`ã€`negative(LABEL_1)`ã¨ã—ã¦å†å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53189385-ef49-45a3-b87a-9955c91395ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", \"japanese\")\n",
    "dataset = dataset.remove_columns([\"source\"])\n",
    "dataset = dataset.filter(lambda dataset: dataset[\"label\"] != 1)\n",
    "dataset = dataset.map(lambda dataset: {\"labels\": int(dataset[\"label\"] == 2)}, remove_columns=[\"label\"])\n",
    "\n",
    "print(dataset[\"train\"][20000])\n",
    "print(dataset[\"train\"][50000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0481f80-52cf-4255-a76e-c49a948247ec",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€æ–‡ç« ãƒ†ã‚­ã‚¹ãƒˆã®ã¾ã¾ã ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã§ããªã„ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³ã®ã‚ã‚‹å˜ä½ã§åˆ†å‰²ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼‰ã—ãŸä¸Šã§æ•°å€¤ã«å¤‰æ›ã—ã¾ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã¯ MeCab ãƒ™ãƒ¼ã‚¹ã® BertJapaneseTokenizer ã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7c32e-c0c0-4a1e-92c2-4b6490f68917",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(4000))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle().select(range(256))\n",
    "\n",
    "# Save dataset\n",
    "train_dataset.save_to_disk(\"./train/\")\n",
    "eval_dataset.save_to_disk(\"./test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4005724-1c52-4426-b7cb-16996462d29d",
   "metadata": {},
   "source": [
    "å®Ÿéš›ã«ã©ã®ã‚ˆã†ã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ã®ã‹ã€ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a9403-e9b7-407e-a6ce-71ecd69697d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 20000\n",
    "print(dataset[\"train\"][index])\n",
    "print('Tokenize:', tokenizer.tokenize(dataset[\"train\"]['text'][index]))\n",
    "print('Encode:', tokenizer.encode(dataset[\"train\"]['text'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b00108-8105-4190-b6ad-27effb7092d5",
   "metadata": {},
   "source": [
    "## Trainer API ã‚’ä½¿ç”¨ã—ãŸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰å®Ÿè¡Œ\n",
    "Huggin Face ğŸ¤—Transformers ã«ã¯ Trainer ã¨ã„ã†ä¾¿åˆ©ãªã‚¯ãƒ©ã‚¹ãŒã‚ã‚Šã€Torch Neuron ã‹ã‚‰ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚ ã“ã“ã§ã¯ Trainer API ã‚’åˆ©ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ã„ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36508a19-4f9d-483c-b118-bbf66b2e85ef",
   "metadata": {},
   "source": [
    "### neuron_parallel_compile ã«ã‚ˆã‚‹äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã‚°ãƒ©ãƒ•ãŒãƒˆãƒ¬ãƒ¼ã‚¹ã•ã‚Œã€ãƒˆãƒ¬ãƒ¼ã‚¹ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ãŒä»¥å‰ã®ã‚‚ã®ã¨ç•°ãªã‚‹å ´åˆã¯ã€å†åº¦è¨ˆç®—ã‚°ãƒ©ãƒ•ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒç™ºç”Ÿã—ã¾ã™ã€‚å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€å„ã‚°ãƒ©ãƒ•ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ãŒé•·ããªã‚‹ã“ã¨ãŒã‚ã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã®ä¸­ã§å ã‚ã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã£ã¦ã—ã¾ã†å ´åˆã‚‚ã‚ã‚Šå¾—ã¾ã™ã€‚ã“ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ãŸã‚ã€PyTorch Neuron ã§ã¯ `neuron_parallel_compile` ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚`neuron_parallel_compile` ã¯ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è©¦è¡Œã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚’æŠ½å‡ºã—ä¸¦åˆ—äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’å®Ÿæ–½ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«çµæœï¼ˆNEFF : Neuron Executable File Formatï¼‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ãƒ‡ã‚£ã‚¹ã‚¯ä¸Šã«ä¿æŒã—ã¾ã™ã€‚\n",
    "\n",
    "ã§ã¯å®Ÿéš›ã«äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä»¥ä¸‹ã®å†…å®¹ã§`bert-jp-precompile.py`ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«åã® Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—å®Ÿè¡Œã—ã¾ã™ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯åŸºæœ¬çš„ã«ã“ã®å¾Œå®Ÿè¡Œã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å†…å®¹ã§ã™ãŒã€`neuron_parallel_compile`ã¯ã‚°ãƒ©ãƒ•ã®é«˜é€Ÿã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã®ã¿ã‚’ç›®çš„ã¨ã—å®Ÿéš›ã®æ¼”ç®—ã¯å®Ÿè¡Œã•ã‚Œãšã€å‡ºåŠ›çµæœã¯ç„¡åŠ¹ã¨ãªã‚Šã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œä¸­ã‚‚å¿…è¦ã«å¿œã˜ã¦ã‚°ãƒ©ãƒ•ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã‚‹ãŸã‚ã€ã“ã®äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ¬¡ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†ã«ç›´æ¥é€²ã‚“ã§ã‚‚å•é¡Œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6acfe7-0dc2-4341-af96-f837bc627583",
   "metadata": {},
   "source": [
    "ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ãŸã‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€epoch æ•°ã‚’åˆ¶é™ã—ã¦ã„ã‚‹ç‚¹ã«ã”æ³¨æ„ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«çµæœã¯ `/var/tmp/neuron-compile-cache/` ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8f769-91a2-4966-8b55-2e1df787d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bert-jp-precompile.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.core.xla_model as xm\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "train_dataset = train_dataset.select(range(64))\n",
    "\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "eval_dataset = eval_dataset.select(range(64))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "eval_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4c135-cd4e-45a3-a2af-99f2a0fbe921",
   "metadata": {},
   "source": [
    "ç’°å¢ƒå¤‰æ•°`XLA_USE_BF16=1`ã€€ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€BF16 + Stochastic Rounding ã§å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db73bea-dcee-4d21-badc-ea7844418195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time XLA_USE_BF16=1 neuron_parallel_compile python3 bert-jp-precompile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958950d5-9135-463f-b87f-3bdea3f9d4b7",
   "metadata": {},
   "source": [
    "äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«ã¯ `inf2.xlarge` ä¸Šã§å®Ÿè¡Œã—ãŸå ´åˆã§ 10åˆ†ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a9a17-1e83-4628-a6f4-29fb5dff2d04",
   "metadata": {},
   "source": [
    "### ã‚·ãƒ³ã‚°ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "æ¬¡ã«å®Ÿéš›ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã™ã€‚äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’å®Ÿè¡Œã—ãŸå ´åˆã§ã‚‚ã€è¿½åŠ ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ä¸€é€šã‚Šã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒçµ‚äº†ã—ãŸå¾Œã€2åº¦ç›®ä»¥é™ã®å®Ÿè¡Œã§ã¯ã€Neuron ã‚³ã‚¢ã®æ©æµã‚’å—ã‘ãŸé«˜é€Ÿãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½“é¨“ã§ãã¾ã™ã€‚ä»¥ä¸‹ã®å†…å®¹ã§ bert-jp-single.py ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«åã® Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "å…ˆç¨‹ã®äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨ã¯ç•°ãªã‚Šã€ä»Šå›ã¯å®Ÿéš›ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã€ç”¨æ„ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ã¦ã«å¯¾ã—ã¦ epoch = 10 ã§å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78747ec6-3e55-4b1a-b57f-a8f656b069a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bert-jp-single.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.core.xla_model as xm\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1319d-30c2-4b9d-be36-07ae7a64fa57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time XLA_USE_BF16=1 python3 bert-jp-single.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae6d30-f843-45fd-b6ef-65a65f61763e",
   "metadata": {},
   "source": [
    "ã‚¹ãƒ†ãƒƒãƒ—æ•° 5000 ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒ 8~9åˆ†ç¨‹ã§å®Œäº†ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œä¸­ã«ã€AWS Neuron ã§æä¾›ã•ã‚Œã‚‹ `neuron-top` ãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ã¨ã€Neuron ã‚³ã‚¢åŠã³ vCPU ã®åˆ©ç”¨ç‡ã€ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ãƒ¡ãƒ¢ãƒªã€ãƒ›ã‚¹ãƒˆãƒ¡ãƒ¢ãƒªã®åˆ©ç”¨çŠ¶æ³ç­‰ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚inf2.xlarge ã«ã¯ã€ä¸€ã¤ã® Inferentia2 ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ãƒãƒƒãƒ—ã€ãƒãƒƒãƒ—å†…ã«äºŒã¤ã® Neuron ã‚³ã‚¢ãŒæ­è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚çµæœã‹ã‚‰ã€äºŒã¤ã‚ã‚‹ Neuron ã‚³ã‚¢ï¼ˆNC0 åŠã³ NC1ï¼‰ã®ã†ã¡ä¸€ã¤ã® Neuron ã‚³ã‚¢ã®ã¿ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚ã¾ã æœ€é©åŒ–ã®ä½™åœ°ã¯ã‚ã‚Šãã†ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bb724-139b-4a2b-9e2c-c66023c65768",
   "metadata": {},
   "source": [
    "ç”Ÿæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœŸå¾…é€šã‚Šã®å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹ã‹ç¢ºèªã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ea0d5-d506-42b5-9f24-0438bc5e742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model = \"./results/\")\n",
    "\n",
    "print(classifier(\"å¤§å¤‰ã™ã°ã‚‰ã—ã„å•†å“ã§ã—ãŸã€‚æ„Ÿæ¿€ã§ã™ã€‚\"))\n",
    "print(classifier(\"æœŸå¾…ã—ã¦ã„ãŸå•†å“ã¨ã¯ç•°ãªã‚Šã¾ã—ãŸã€‚æ®‹å¿µã§ã™ã€‚\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfdbb2d-d477-42e3-9967-f64fba883b0d",
   "metadata": {},
   "source": [
    "æœŸå¾…é€šã‚Šã®å‡ºåŠ›ã‚’å¾—ã‚‰ã‚Œã‚‹ã“ã¨ãŒç¢ºèªã§ããŸã‚ˆã†ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28270199-5a61-4ec1-a045-833abc6c16e7",
   "metadata": {},
   "source": [
    "## torchrun ã‚’ç”¨ã„ãŸãƒãƒ«ãƒãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "ãã‚Œã§ã¯ã€å…ˆç¨‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«å¤‰æ›´ã‚’åŠ ãˆã€äºŒã¤ã‚ã‚‹ Neuron ã‚³ã‚¢ã‚’æœ‰åŠ¹æ´»ç”¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚è¤‡æ•°ã® Neuron ã‚³ã‚¢ã‚’åˆ©ç”¨ã—ãŸãƒãƒ«ãƒãƒ¯ãƒ¼ã‚«ãƒ¼ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã¯ `torchrun` ã‚³ãƒãƒ³ãƒ‰ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚`torchrun` ã‚³ãƒãƒ³ãƒ‰ã«å¯¾ã—ã¦ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ `--nproc_per_node` ã§åˆ©ç”¨ã™ã‚‹ Neuron ã‚³ã‚¢ã®æ•°ï¼ˆä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚trn1.2xlarge (inf2.xlarge) ã§ã¯ 2 ã‚’ã€trn1.32xlargeã®å ´åˆã¯ 2, 8, 32 ãŒæŒ‡å®šå¯èƒ½ã§ã™ã€‚\n",
    "\n",
    "`torchrun` ã‚’åˆ©ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¬ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã«ã‚ãŸã£ã¦ã€å…ˆç¨‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ä¸€éƒ¨å¤‰æ›´ã‚’åŠ ãˆãŸ `bert-jp-dual.py` ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "ãã‚Œã§ã¯å¤‰æ›´å¾Œã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åˆ©ç”¨ã—ã¦ã€€inf2.xlarge ä¸Šã®äºŒã¤ Neuron ã‚³ã‚¢ã‚’åˆ©ç”¨ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã‚·ãƒ³ã‚°ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã¨æ¯”è¼ƒã— `Total train batch size` ã®å€¤ãŒå€ã® 16 ã«ã€`Total optimization steps` ãŒåŠåˆ†ã® 2500 ã¨ãªã£ã¦ã„ã‚‹ç‚¹ã‚’ç¢ºèªã§ãã‚‹ã¨æ€ã„ã¾ã™ã€‚\n",
    "\n",
    "ã‚·ãƒ³ã‚°ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼æ™‚ã®æ‰‹é †åŒæ§˜ã€ã¾ãšã¯äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’å®Ÿè¡Œã—ã€ãã®å¾Œã«å®Ÿéš›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2837ccc-aac9-47d7-803e-3c75afccc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bert-jp-dual-precompile.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.distributed.xla_backend\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "train_dataset = train_dataset.select(range(64))\n",
    "\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "eval_dataset = eval_dataset.select(range(64))\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "eval_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d164bb-deb5-4c34-bafa-8c66b2291ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time XLA_USE_BF16=1 neuron_parallel_compile torchrun --nproc_per_node=2 bert-jp-dual-precompile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bddfc6-2cb6-4383-b095-e6cb58a9e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bert-jp-dual.py\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import torch, torch_xla.distributed.xla_backend\n",
    "import os\n",
    "\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = \"--model-type=transformer\"\n",
    "\n",
    "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "train_dataset = load_from_disk(\"./train/\").with_format(\"torch\")\n",
    "eval_dataset = load_from_disk(\"./test/\").with_format(\"torch\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 10,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    output_dir = \"./results\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d8ec0-162b-4d6a-bebf-a0b991b08213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time XLA_USE_BF16=1 torchrun --nproc_per_node=2 bert-jp-dual.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582fc974-a3fa-45ad-ba0b-1d3ac99d8f33",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œä¸­ã® neuron-top ã®å‡ºåŠ›ã‚‚ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä»Šåº¦ã¯äºŒã¤ã® Neuron ã‚³ã‚¢ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹äº‹ãŒç¢ºèªã§ãã‚‹ã¨æ€ã„ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«è¦ã™ã‚‹å®Ÿè¡Œæ™‚é–“ã‚‚ 5~6 åˆ†ã«å‰Šæ¸›ã•ã‚Œã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c4c41-9152-4b02-91fa-d5897079427f",
   "metadata": {},
   "source": [
    "## æ¨è«–å®Ÿè¡Œ\n",
    "å…ˆã»ã©ã¯ç”Ÿæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœŸå¾…é€šã‚Šã®å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹ã‹ã©ã†ã‹CPUä¸Šã§æ¨è«–å®Ÿè¡Œã—ã€çµæœã‚’ç¢ºèªã—ã¾ã—ãŸã€‚ã“ã“ã§ã¯ç”Ÿæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’inf2.xlargeä¸Šã§æ¨è«–å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b881d0f-28b5-49e5-849f-3f0266daa97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "\n",
    "def encode(tokenizer, *inputs, max_length=128, batch_size=1):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        *inputs,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return (\n",
    "        torch.repeat_interleave(tokens['input_ids'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['attention_mask'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['token_type_ids'], batch_size, 0),\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/\", torchscript=True)\n",
    "\n",
    "\n",
    "sequence = \"å¤§å¤‰ã™ã°ã‚‰ã—ã„å•†å“ã§ã—ãŸã€‚æ„Ÿæ¿€ã§ã™ã€‚\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "cpu_paraphrase_logits = model(*paraphrase)[0]\n",
    "print('CPU paraphrase logits:', cpu_paraphrase_logits.detach().numpy())\n",
    "\n",
    "sequence = \"æœŸå¾…ã—ã¦ã„ãŸå•†å“ã¨ã¯ç•°ãªã‚Šã¾ã—ãŸã€‚æ®‹å¿µã§ã™ã€‚\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "cpu_paraphrase_logits = model(*paraphrase)[0]\n",
    "print('CPU paraphrase logits:', cpu_paraphrase_logits.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e608f7d-16cb-4dc0-b7ac-253129e64df2",
   "metadata": {},
   "source": [
    "### Compile the model for Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc7d66-7fdb-42d8-91f3-98708982fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee754ea0-97ae-4cd4-8184-4cd3b7fa6fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_neuron = torch_neuronx.trace(model, paraphrase)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f805991-1929-4899-aa6d-46682b7e3acb",
   "metadata": {},
   "source": [
    "### Run inference and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e935f-0eab-4128-85db-047e5022e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "sequence = \"å¤§å¤‰ã™ã°ã‚‰ã—ã„å•†å“ã§ã—ãŸã€‚æ„Ÿæ¿€ã§ã™ã€‚\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "neuron_paraphrase_logits = model_neuron(*paraphrase)[0]\n",
    "print('Neuron paraphrase logits:', neuron_paraphrase_logits.detach().numpy())\n",
    "\n",
    "sequence = \"æœŸå¾…ã—ã¦ã„ãŸå•†å“ã¨ã¯ç•°ãªã‚Šã¾ã—ãŸã€‚æ®‹å¿µã§ã™ã€‚\"\n",
    "paraphrase = encode(tokenizer, sequence)\n",
    "neuron_paraphrase_logits = model_neuron(*paraphrase)[0]\n",
    "print('Neuron paraphrase logits:', neuron_paraphrase_logits.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3299b-f8ec-4083-8813-54b693270cb0",
   "metadata": {},
   "source": [
    "CPUã§æ¨è«–å®Ÿè¡Œã—ãŸçµæœã¨åŒæ§˜ã®çµæœãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹äº‹ãŒç¢ºèªã§ãã¾ã—ãŸã€‚æ¨è«–æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹æ–¹æ³•ã¯ä»¥ä¸‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã”å‚ç…§ä¸‹ã•ã„ã€‚\n",
    "+ https://github.com/aws-neuron/aws-neuron-sdk/blob/master/src/examples/pytorch/torch-neuronx/bert-base-cased-finetuned-mrpc-inference-on-trn1-tutorial.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
