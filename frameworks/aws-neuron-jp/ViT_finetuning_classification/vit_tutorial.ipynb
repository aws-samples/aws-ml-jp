{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4718e6d-7761-415a-87a1-8946df7087f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ViT Model Fine-tuning & Deployment on Inferentia2/Trainium\n",
    "\n",
    "ViT ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯ç”¨ã«è¨­è¨ˆã•ã‚ŒãŸ transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ããƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "\n",
    "ImageNet-21K ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸã€€ViT ãƒ¢ãƒ‡ãƒ«ã‚’ã€Beans ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚\n",
    "ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€æ•°ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€Beans(è‘‰)ã®å¥åº·çŠ¶æ…‹ã‚’3ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¦äºˆæ¸¬å¯èƒ½ã§ã™ã€‚ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c27b4-1c5f-4295-8b48-ad423283f1b1",
   "metadata": {},
   "source": [
    "## äº‹å‰æº–å‚™\n",
    "æœ¬ notebookã¯ Neuron 2.13.2 ç’°å¢ƒä¸‹ã§å‹•ä½œç¢ºèªã—ã¦ã„ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115c03c9-5b62-4cd6-9a93-657a95e337ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (23.2.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.31.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (4.31.0)\n",
      "Requirement already satisfied: accelerate in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (0.22.0)\n",
      "Requirement already satisfied: evaluate in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: gradio in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (3.42.0)\n",
      "Requirement already satisfied: filelock in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from transformers==4.31.0) (4.66.1)\n",
      "Requirement already satisfied: psutil in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: dill in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (5.1.1)\n",
      "Requirement already satisfied: fastapi in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.101.1)\n",
      "Requirement already satisfied: ffmpy in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.5.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: httpx in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.24.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (3.7.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (3.9.5)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (10.0.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: pydub in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (4.7.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (0.23.2)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
      "Requirement already satisfied: toolz in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (13.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.16.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (2.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers==4.31.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers==4.31.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (68.1.0)\n",
      "Requirement already satisfied: wheel in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from httpx->gradio) (0.17.3)\n",
      "Requirement already satisfied: sniffio in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U transformers==4.31.0 accelerate evaluate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3324f68f-1307-4ef4-99c8-b0d2414bbea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuronx-runtime-discovery 2.9\n",
      "libneuronxla                  0.5.413\n",
      "neuronx-cc                    2.8.0.25+a3ad0f342\n",
      "neuronx-hwm                   2.8.0.3+2b7c6da39\n",
      "torch                         1.13.1\n",
      "torch-neuronx                 1.13.1.1.9.0\n",
      "torch-xla                     1.13.1+torchneuron8\n",
      "torchvision                   0.14.1\n",
      "transformers                  4.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep \"neuron\\|torch\\|transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf201a69-ce0a-493d-86dd-565d2c067a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi  aws-neuronx-collectives                2.15.16.0-db4e2d9a9               amd64        neuron_ccom built using CMake\n",
      "hi  aws-neuronx-dkms                       2.11.9.0                          amd64        aws-neuronx driver in DKMS format.\n",
      "hi  aws-neuronx-oci-hook                   2.2.16.0                          amd64        neuron_oci_hook built using CMake\n",
      "hi  aws-neuronx-runtime-lib                2.15.14.0-279f319f2               amd64        neuron_runtime built using CMake\n",
      "hi  aws-neuronx-tools                      2.12.2.0                          amd64        Neuron profile and debug tools\n"
     ]
    }
   ],
   "source": [
    "!dpkg --list | grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d12f43b-ee0c-4bcc-bda7-b62526202391",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo rmmod neuron; sudo modprobe neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde899b-8e68-4cfe-8708-6a586e077082",
   "metadata": {},
   "source": [
    "## Trainer API ã‚’ä½¿ç”¨ã—ãŸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰å®Ÿè¡Œ\n",
    "Transformers ã«ã¯ Trainer ã¨ã„ã†ä¾¿åˆ©ãªã‚¯ãƒ©ã‚¹ãŒã‚ã‚Šã€Torch Neuron ã‹ã‚‰ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚ ã“ã“ã§ã¯ Trainer API ã‚’åˆ©ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ã„ãã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1daf4b96-f36b-4d67-aeee-2bc5d3f160b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env python\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# coding=utf-8\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# you may not use this file except in compliance with the License.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# You may obtain a copy of the License at\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#     http://www.apache.org/licenses/LICENSE-2.0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Unless required by applicable law or agreed to in writing, software\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# See the License for the specific language governing permissions and\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataclasses\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m dataclass, field\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Optional\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mevaluate\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Image\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    CenterCrop,\u001b[37m\u001b[39;49;00m\n",
      "    Compose,\u001b[37m\u001b[39;49;00m\n",
      "    Normalize,\u001b[37m\u001b[39;49;00m\n",
      "    RandomHorizontalFlip,\u001b[37m\u001b[39;49;00m\n",
      "    RandomResizedCrop,\u001b[37m\u001b[39;49;00m\n",
      "    Resize,\u001b[37m\u001b[39;49;00m\n",
      "    ToTensor,\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\u001b[37m\u001b[39;49;00m\n",
      "    AutoConfig,\u001b[37m\u001b[39;49;00m\n",
      "    AutoImageProcessor,\u001b[37m\u001b[39;49;00m\n",
      "    AutoModelForImageClassification,\u001b[37m\u001b[39;49;00m\n",
      "    HfArgumentParser,\u001b[37m\u001b[39;49;00m\n",
      "    Trainer,\u001b[37m\u001b[39;49;00m\n",
      "    TrainingArguments,\u001b[37m\u001b[39;49;00m\n",
      "    set_seed,\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtrainer_utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_last_checkpoint\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m check_min_version, send_example_telemetry\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mversions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m require_version\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\" Fine-tuning a ğŸ¤— Transformers model for image classification\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "check_min_version(\u001b[33m\"\u001b[39;49;00m\u001b[33m4.27.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "require_version(\u001b[33m\"\u001b[39;49;00m\u001b[33mdatasets>=1.8.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mTo fix: pip install -r examples/pytorch/image-classification/requirements.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "MODEL_CONFIG_CLASSES = \u001b[36mlist\u001b[39;49;00m(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING.keys())\u001b[37m\u001b[39;49;00m\n",
      "MODEL_TYPES = \u001b[36mtuple\u001b[39;49;00m(conf.model_type \u001b[34mfor\u001b[39;49;00m conf \u001b[35min\u001b[39;49;00m MODEL_CONFIG_CLASSES)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpil_loader\u001b[39;49;00m(path: \u001b[36mstr\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(path, \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "        im = Image.open(f)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m im.convert(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[90m@dataclass\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDataTrainingArguments\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Arguments pertaining to what data we are going to input our model for training and eval.\u001b[39;49;00m\n",
      "\u001b[33m    Using `HfArgumentParser` we can turn this class into argparse arguments to be able to specify\u001b[39;49;00m\n",
      "\u001b[33m    them on the command line.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dataset_name: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mName of a dataset from the hub (could be your own, possibly private dataset hosted on the hub).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    dataset_config_name: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mThe configuration name of the dataset to use (via the datasets library).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    train_dir: Optional[\u001b[36mstr\u001b[39;49;00m] = field(default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mA folder containing the training data.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[37m\u001b[39;49;00m\n",
      "    validation_dir: Optional[\u001b[36mstr\u001b[39;49;00m] = field(default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mA folder containing the validation data.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[37m\u001b[39;49;00m\n",
      "    train_val_split: Optional[\u001b[36mfloat\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m0.15\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mPercent to split off of train for validation.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    max_train_samples: Optional[\u001b[36mint\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mFor debugging purposes or quicker training, truncate the number of training examples to this \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue if set.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    max_eval_samples: Optional[\u001b[36mint\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mFor debugging purposes or quicker training, truncate the number of evaluation examples to this \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue if set.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__post_init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.dataset_name \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.train_dir \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.validation_dir \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mYou must specify either a dataset name from the hub or a train and/or validation directory.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[90m@dataclass\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mModelArguments\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_name_or_path: \u001b[36mstr\u001b[39;49;00m = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33mgoogle/vit-base-patch16-224-in21k\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mPath to pretrained model or model identifier from huggingface.co/models\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model_type: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mIf training from scratch, pass a model type from the list: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[33m\"\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(MODEL_TYPES)},\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    config_name: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mPretrained config name or path if not the same as model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    cache_dir: Optional[\u001b[36mstr\u001b[39;49;00m] = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mWhere do you want to store the pretrained models downloaded from s3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model_revision: \u001b[36mstr\u001b[39;49;00m = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mThe specific model version to use (can be a branch name, tag name or commit id).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    image_processor_name: \u001b[36mstr\u001b[39;49;00m = field(default=\u001b[34mNone\u001b[39;49;00m, metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mName or path of preprocessor config.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[37m\u001b[39;49;00m\n",
      "    use_auth_token: \u001b[36mbool\u001b[39;49;00m = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mWill use the token generated when running `huggingface-cli login` (necessary to use this script \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mwith private models).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    ignore_mismatched_sizes: \u001b[36mbool\u001b[39;49;00m = field(\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metadata={\u001b[33m\"\u001b[39;49;00m\u001b[33mhelp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mWill enable to load a pretrained model whose head dimensions are different.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcollate_fn\u001b[39;49;00m(examples):\u001b[37m\u001b[39;49;00m\n",
      "    pixel_values = torch.stack([example[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m example \u001b[35min\u001b[39;49;00m examples])\u001b[37m\u001b[39;49;00m\n",
      "    labels = torch.tensor([example[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m example \u001b[35min\u001b[39;49;00m examples])\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: pixel_values, \u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: labels}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# See all possible arguments in src/transformers/training_args.py\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# or by passing the --help flag to this script.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# We now keep distinct sets of args, for a cleaner separation of concerns.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(sys.argv) == \u001b[34m2\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m sys.argv[\u001b[34m1\u001b[39;49;00m].endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# If we pass only one argument to the script and it's the path to a json file,\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# let's parse it to get our arguments.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[\u001b[34m1\u001b[39;49;00m]))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# information sent is the one passed as arguments along with your Python/PyTorch versions.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    send_example_telemetry(\u001b[33m\"\u001b[39;49;00m\u001b[33mrun_image_classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, model_args, data_args)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Setup logging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logging.basicConfig(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        datefmt=\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm/\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY \u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.should_log:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# The default of training_args.log_level is passive, so we set log level at info here to have that default.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        transformers.utils.logging.set_verbosity_info()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    log_level = training_args.get_process_log_level()\u001b[37m\u001b[39;49;00m\n",
      "    logger.setLevel(log_level)\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.set_verbosity(log_level)\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.enable_default_handler()\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.enable_explicit_format()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Log on each process the small summary:\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger.warning(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mProcess rank: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.local_rank\u001b[33m}\u001b[39;49;00m\u001b[33m, device: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.device\u001b[33m}\u001b[39;49;00m\u001b[33m, n_gpu: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.n_gpu\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        + \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mdistributed training: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mbool\u001b[39;49;00m(training_args.local_rank\u001b[37m \u001b[39;49;00m!=\u001b[37m \u001b[39;49;00m-\u001b[34m1\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m, 16-bits training: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.fp16\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining/evaluation parameters \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Detecting last checkpoint.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    last_checkpoint = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m os.path.isdir(training_args.output_dir) \u001b[35mand\u001b[39;49;00m training_args.do_train \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m training_args.overwrite_output_dir:\u001b[37m\u001b[39;49;00m\n",
      "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m last_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(os.listdir(training_args.output_dir)) > \u001b[34m0\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mOutput directory (\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_args.output_dir\u001b[33m}\u001b[39;49;00m\u001b[33m) already exists and is not empty. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mUse --overwrite_output_dir to overcome.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melif\u001b[39;49;00m last_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m training_args.resume_from_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            logger.info(\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCheckpoint detected, resuming training at \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlast_checkpoint\u001b[33m}\u001b[39;49;00m\u001b[33m. To avoid this behavior, change \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mthe `--output_dir` or add `--overwrite_output_dir` to train from scratch.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set seed before initializing model.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    set_seed(training_args.seed)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Initialize our dataset and prepare it for the 'image-classification' task.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m data_args.dataset_name \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        dataset = load_dataset(\u001b[37m\u001b[39;49;00m\n",
      "            data_args.dataset_name,\u001b[37m\u001b[39;49;00m\n",
      "            data_args.dataset_config_name,\u001b[37m\u001b[39;49;00m\n",
      "            cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "            task=\u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            use_auth_token=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m model_args.use_auth_token \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        data_files = {}\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m data_args.train_dir \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            data_files[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = os.path.join(data_args.train_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m data_args.validation_dir \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            data_files[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = os.path.join(data_args.validation_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        dataset = load_dataset(\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mimagefolder\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            data_files=data_files,\u001b[37m\u001b[39;49;00m\n",
      "            cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "            task=\u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# If we don't have a validation split, split off a percentage of train as validation.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    data_args.train_val_split = \u001b[34mNone\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m dataset.keys() \u001b[34melse\u001b[39;49;00m data_args.train_val_split\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data_args.train_val_split, \u001b[36mfloat\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m data_args.train_val_split > \u001b[34m0.0\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        split = dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].train_test_split(data_args.train_val_split)\u001b[37m\u001b[39;49;00m\n",
      "        dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = split[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = split[\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Prepare label mappings.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# We'll include these in the model's config to get human readable labels in the Inference API.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    labels = dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].names\u001b[37m\u001b[39;49;00m\n",
      "    label2id, id2label = {}, {}\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m i, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(labels):\u001b[37m\u001b[39;49;00m\n",
      "        label2id[label] = \u001b[36mstr\u001b[39;49;00m(i)\u001b[37m\u001b[39;49;00m\n",
      "        id2label[\u001b[36mstr\u001b[39;49;00m(i)] = label\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load the accuracy metric from the datasets package\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    metric = evaluate.load(\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Define our compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# predictions and label_ids field) and has to return a dictionary string to float.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(p):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[33m\"\"\"Computes accuracy on a batch of predictions\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m metric.compute(predictions=np.argmax(p.predictions, axis=\u001b[34m1\u001b[39;49;00m), references=p.label_ids)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    config = AutoConfig.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_args.config_name \u001b[35mor\u001b[39;49;00m model_args.model_name_or_path,\u001b[37m\u001b[39;49;00m\n",
      "        num_labels=\u001b[36mlen\u001b[39;49;00m(labels),\u001b[37m\u001b[39;49;00m\n",
      "        label2id=label2id,\u001b[37m\u001b[39;49;00m\n",
      "        id2label=id2label,\u001b[37m\u001b[39;49;00m\n",
      "        finetuning_task=\u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "        revision=model_args.model_revision,\u001b[37m\u001b[39;49;00m\n",
      "        use_auth_token=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m model_args.use_auth_token \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForImageClassification.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_args.model_name_or_path,\u001b[37m\u001b[39;49;00m\n",
      "        from_tf=\u001b[36mbool\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m.ckpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m model_args.model_name_or_path),\u001b[37m\u001b[39;49;00m\n",
      "        config=config,\u001b[37m\u001b[39;49;00m\n",
      "        cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "        revision=model_args.model_revision,\u001b[37m\u001b[39;49;00m\n",
      "        use_auth_token=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m model_args.use_auth_token \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    image_processor = AutoImageProcessor.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_args.image_processor_name \u001b[35mor\u001b[39;49;00m model_args.model_name_or_path,\u001b[37m\u001b[39;49;00m\n",
      "        cache_dir=model_args.cache_dir,\u001b[37m\u001b[39;49;00m\n",
      "        revision=model_args.model_revision,\u001b[37m\u001b[39;49;00m\n",
      "        use_auth_token=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m model_args.use_auth_token \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Define torchvision transforms to be applied to each image.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mshortest_edge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m image_processor.size:\u001b[37m\u001b[39;49;00m\n",
      "        size = image_processor.size[\u001b[33m\"\u001b[39;49;00m\u001b[33mshortest_edge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        size = (image_processor.size[\u001b[33m\"\u001b[39;49;00m\u001b[33mheight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], image_processor.size[\u001b[33m\"\u001b[39;49;00m\u001b[33mwidth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\u001b[37m\u001b[39;49;00m\n",
      "    _train_transforms = Compose(\u001b[37m\u001b[39;49;00m\n",
      "        [\u001b[37m\u001b[39;49;00m\n",
      "            RandomResizedCrop(size),\u001b[37m\u001b[39;49;00m\n",
      "            RandomHorizontalFlip(),\u001b[37m\u001b[39;49;00m\n",
      "            ToTensor(),\u001b[37m\u001b[39;49;00m\n",
      "            normalize,\u001b[37m\u001b[39;49;00m\n",
      "        ]\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    _val_transforms = Compose(\u001b[37m\u001b[39;49;00m\n",
      "        [\u001b[37m\u001b[39;49;00m\n",
      "            Resize(size),\u001b[37m\u001b[39;49;00m\n",
      "            CenterCrop(size),\u001b[37m\u001b[39;49;00m\n",
      "            ToTensor(),\u001b[37m\u001b[39;49;00m\n",
      "            normalize,\u001b[37m\u001b[39;49;00m\n",
      "        ]\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_transforms\u001b[39;49;00m(example_batch):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[33m\"\"\"Apply _train_transforms across a batch.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        example_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = [\u001b[37m\u001b[39;49;00m\n",
      "            _train_transforms(pil_img.convert(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \u001b[34mfor\u001b[39;49;00m pil_img \u001b[35min\u001b[39;49;00m example_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        ]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m example_batch\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mval_transforms\u001b[39;49;00m(example_batch):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[33m\"\"\"Apply _val_transforms across a batch.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        example_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = [_val_transforms(pil_img.convert(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \u001b[34mfor\u001b[39;49;00m pil_img \u001b[35min\u001b[39;49;00m example_batch[\u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m example_batch\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.do_train:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m dataset:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m--do_train requires a train dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m data_args.max_train_samples \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = (\u001b[37m\u001b[39;49;00m\n",
      "                dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].shuffle(seed=training_args.seed).select(\u001b[36mrange\u001b[39;49;00m(data_args.max_train_samples))\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Set the training transforms\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].set_transform(train_transforms)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.do_eval:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m dataset:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m--do_eval requires a validation dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m data_args.max_eval_samples \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = (\u001b[37m\u001b[39;49;00m\n",
      "                dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].shuffle(seed=training_args.seed).select(\u001b[36mrange\u001b[39;49;00m(data_args.max_eval_samples))\u001b[37m\u001b[39;49;00m\n",
      "            )\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Set the validation transforms\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].set_transform(val_transforms)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Initalize our trainer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer = Trainer(\u001b[37m\u001b[39;49;00m\n",
      "        model=model,\u001b[37m\u001b[39;49;00m\n",
      "        args=training_args,\u001b[37m\u001b[39;49;00m\n",
      "        train_dataset=dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m training_args.do_train \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        eval_dataset=dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m training_args.do_eval \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        compute_metrics=compute_metrics,\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer=image_processor,\u001b[37m\u001b[39;49;00m\n",
      "        data_collator=collate_fn,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Training\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.do_train:\u001b[37m\u001b[39;49;00m\n",
      "        checkpoint = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m training_args.resume_from_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            checkpoint = training_args.resume_from_checkpoint\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melif\u001b[39;49;00m last_checkpoint \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            checkpoint = last_checkpoint\u001b[37m\u001b[39;49;00m\n",
      "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.save_model()\u001b[37m\u001b[39;49;00m\n",
      "        trainer.log_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train_result.metrics)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.save_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train_result.metrics)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.save_state()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Evaluation\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.do_eval:\u001b[37m\u001b[39;49;00m\n",
      "        metrics = trainer.evaluate()\u001b[37m\u001b[39;49;00m\n",
      "        trainer.log_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33meval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, metrics)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.save_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33meval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, metrics)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Write model card and (optionally) push to hub\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    kwargs = {\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mfinetuned_from\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_args.model_name_or_path,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtasks\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: data_args.dataset_name,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: [\u001b[33m\"\u001b[39;49;00m\u001b[33mimage-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mvision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m training_args.push_to_hub:\u001b[37m\u001b[39;49;00m\n",
      "        trainer.push_to_hub(**kwargs)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trainer.create_model_card(**kwargs)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    main()\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize run_image_classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861405a-694d-4018-ae79-abe6c26d5aa5",
   "metadata": {},
   "source": [
    "- HuggingFace transformers API ã‚’ä½¿ç”¨ã—ã¦ ViT ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚\n",
    "- Neuron ã‚³ã‚¢ä¸Šã§å®Ÿè¡Œã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿å‹ã¯ã€ã‚ˆã‚ŠåŠ¹ç‡ã‚’é«˜ã‚ã‚‹ãŸã‚ã« `fp32` ã§ã¯ãªã `bf16` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãŒä¿å­˜ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ`./compiler_cache`ï¼‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "- PyTorchã® `torchrun` ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’èµ·å‹•ã—ã¾ã™ã€‚\n",
    "- Inferentia2 (ã‚‚ã—ãã¯ Trainium) ãƒãƒƒãƒ—ã‚’ ï¼‘ã¤æ­è¼‰ã—ãŸã€€Inf2.xlarge (ã‚‚ã—ãã¯ Trn1.2xlarge) ä¸Šã§ã®å®Ÿè¡Œã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚å„ãƒãƒƒãƒ—ã¯ ï¼’ ã¤ã® Neuron ã‚³ã‚¢ã‚’æ­è¼‰ã—ã¦ã„ã‚‹ãŸã‚ `num_workers=2` ã¨è¨­å®šã€çµæœã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã¯ 2ã¤ã® Neuron ã‚³ã‚¢ä¸Šã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚\n",
    "- ãƒ¢ãƒ‡ãƒ«ã‚’ 10 ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã—ã€ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚ä¿å­˜ã§ãã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ 1 ã¤ã¾ã§ã§ã™ã€‚ãƒ­ã‚®ãƒ³ã‚°æƒ…å ±ã¯ 10 å›ã”ã¨ã«å‡ºåŠ›ã—ã¾ã™ã€‚\n",
    "-ã€€`./output` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ç”Ÿæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã€Configã€ãã®ä»–ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãŒæ ¼ç´ã•ã‚Œã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27686c3c-1829-465d-b656-1c399755d401",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "WARNING:__main__:Process rank: 0, device: xla:1, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/runs/Sep07_05-13-04_ip-172-31-11-245,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=1,\n",
      "seed=1337,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:__main__:Process rank: 1, device: xla:0, n_gpu: 0distributed training: True, 16-bits training: False\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/datasets/load.py:2103: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
      "\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:712] 2023-09-07 05:13:05,958 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-09-07 05:13:05,958 >> Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"angular_leaf_spot\",\n",
      "    \"1\": \"bean_rust\",\n",
      "    \"2\": \"healthy\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"angular_leaf_spot\": \"0\",\n",
      "    \"bean_rust\": \"1\",\n",
      "    \"healthy\": \"2\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2603] 2023-09-07 05:13:05,960 >> loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:3331] 2023-09-07 05:13:06,560 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|modeling_utils.py:3319] 2023-09-07 05:13:06,565 >> Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3331] 2023-09-07 05:13:06,566 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|image_processing_utils.py:339] 2023-09-07 05:13:06,671 >> loading configuration file preprocessor_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:712] 2023-09-07 05:13:06,762 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/7cbdb7ee3a6bcdf99dae654893f66519c480a0f8/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-09-07 05:13:06,762 >> Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "[INFO|image_processing_utils.py:572] 2023-09-07 05:13:06,852 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:389] 2023-09-07 05:13:06,852 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1686] 2023-09-07 05:13:17,356 >> ***** Running training *****\n",
      "[INFO|trainer.py:1687] 2023-09-07 05:13:17,356 >>   Num examples = 528\n",
      "[INFO|trainer.py:1688] 2023-09-07 05:13:17,356 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1689] 2023-09-07 05:13:17,356 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1692] 2023-09-07 05:13:17,356 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1693] 2023-09-07 05:13:17,356 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1694] 2023-09-07 05:13:17,356 >>   Total optimization steps = 330\n",
      "[INFO|trainer.py:1695] 2023-09-07 05:13:17,357 >>   Number of trainable parameters = 85,800,963\n",
      "  0%|                                                   | 0/330 [00:00<?, ?it/s]2023-09-07 05:13:17.000493: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:17.000495: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_10186977725140102164+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-Sep-07 05:13:17.0528 50288:50327 [0] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2023-Sep-07 05:13:17.0528 50288:50327 [0] init.cc:138 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "  0%|â–                                          | 1/330 [00:00<04:03,  1.35it/s]2023-09-07 05:13:19.000062: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:19.000314: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_7440857343363989218+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  1%|â–                                          | 2/330 [00:02<08:12,  1.50s/it]2023-09-07 05:13:23.000897: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:24.000162: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_6358742223045178700+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  3%|â–ˆâ–                                        | 10/330 [00:11<02:07,  2.51it/s]2023-09-07 05:13:29.000286: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:29.000287: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_569571592325889951+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 1.025, 'learning_rate': 1.9393939393939395e-05, 'epoch': 0.3}          \n",
      "  3%|â–ˆâ–                                        | 10/330 [00:11<02:07,  2.51it/s]2023-09-07 05:13:29.000454: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:29.000455: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_931450859643720011+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 0.8656, 'learning_rate': 1.8787878787878792e-05, 'epoch': 0.61}        \n",
      "{'loss': 0.7344, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.91}        \n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 33/330 [00:15<00:39,  7.49it/s][INFO|trainer.py:2777] 2023-09-07 05:13:32,588 >> Saving model checkpoint to output/checkpoint-33\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:32,590 >> Configuration saved in output/checkpoint-33/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:13:33,308 >> Model weights saved in output/checkpoint-33/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:13:33,309 >> Image processor saved in output/checkpoint-33/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:13:34,670 >> Deleting older checkpoint [output/checkpoint-330] due to args.save_total_limit\n",
      "2023-09-07 05:13:34.000812: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:13:34.000813: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_2784763037766117603+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "{'loss': 0.6047, 'learning_rate': 1.7575757575757576e-05, 'epoch': 1.21}        \n",
      "{'loss': 0.4953, 'learning_rate': 1.6969696969696972e-05, 'epoch': 1.52}        \n",
      "{'loss': 0.4023, 'learning_rate': 1.6363636363636366e-05, 'epoch': 1.82}        \n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 66/330 [00:22<00:33,  7.89it/s][INFO|trainer.py:2777] 2023-09-07 05:13:39,652 >> Saving model checkpoint to output/checkpoint-66\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:39,654 >> Configuration saved in output/checkpoint-66/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:13:40,315 >> Model weights saved in output/checkpoint-66/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:13:40,316 >> Image processor saved in output/checkpoint-66/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:13:41,527 >> Deleting older checkpoint [output/checkpoint-33] due to args.save_total_limit\n",
      "{'loss': 0.3484, 'learning_rate': 1.575757575757576e-05, 'epoch': 2.12}         \n",
      "{'loss': 0.2797, 'learning_rate': 1.5151515151515153e-05, 'epoch': 2.42}        \n",
      "{'loss': 0.2445, 'learning_rate': 1.4545454545454546e-05, 'epoch': 2.73}        \n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 99/330 [00:28<00:29,  7.96it/s][INFO|trainer.py:2777] 2023-09-07 05:13:46,380 >> Saving model checkpoint to output/checkpoint-99\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:46,382 >> Configuration saved in output/checkpoint-99/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:13:46,977 >> Model weights saved in output/checkpoint-99/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:13:46,978 >> Image processor saved in output/checkpoint-99/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:13:48,045 >> Deleting older checkpoint [output/checkpoint-66] due to args.save_total_limit\n",
      "{'loss': 0.2625, 'learning_rate': 1.3939393939393942e-05, 'epoch': 3.03}        \n",
      "{'loss': 0.2117, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.33}        \n",
      "{'loss': 0.1664, 'learning_rate': 1.2727272727272728e-05, 'epoch': 3.64}        \n",
      "{'loss': 0.1762, 'learning_rate': 1.2121212121212122e-05, 'epoch': 3.94}        \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 131/330 [00:35<00:32,  6.10it/s][INFO|trainer.py:2777] 2023-09-07 05:13:53,019 >> Saving model checkpoint to output/checkpoint-132\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:53,023 >> Configuration saved in output/checkpoint-132/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:13:53,676 >> Model weights saved in output/checkpoint-132/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:13:53,676 >> Image processor saved in output/checkpoint-132/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:13:54,913 >> Deleting older checkpoint [output/checkpoint-99] due to args.save_total_limit\n",
      "{'loss': 0.1566, 'learning_rate': 1.1515151515151517e-05, 'epoch': 4.24}        \n",
      "{'loss': 0.1512, 'learning_rate': 1.0909090909090909e-05, 'epoch': 4.55}        \n",
      "{'loss': 0.1457, 'learning_rate': 1.0303030303030304e-05, 'epoch': 4.85}        \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 165/330 [00:42<00:21,  7.73it/s][INFO|trainer.py:2777] 2023-09-07 05:13:59,778 >> Saving model checkpoint to output/checkpoint-165\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:13:59,780 >> Configuration saved in output/checkpoint-165/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:00,433 >> Model weights saved in output/checkpoint-165/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:00,433 >> Image processor saved in output/checkpoint-165/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:01,579 >> Deleting older checkpoint [output/checkpoint-132] due to args.save_total_limit\n",
      "{'loss': 0.1504, 'learning_rate': 9.696969696969698e-06, 'epoch': 5.15}         \n",
      "{'loss': 0.1418, 'learning_rate': 9.090909090909091e-06, 'epoch': 5.45}         \n",
      "{'loss': 0.1309, 'learning_rate': 8.484848484848486e-06, 'epoch': 5.76}         \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 198/330 [00:48<00:16,  7.90it/s][INFO|trainer.py:2777] 2023-09-07 05:14:06,445 >> Saving model checkpoint to output/checkpoint-198\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:06,447 >> Configuration saved in output/checkpoint-198/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:07,110 >> Model weights saved in output/checkpoint-198/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:07,111 >> Image processor saved in output/checkpoint-198/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:08,303 >> Deleting older checkpoint [output/checkpoint-165] due to args.save_total_limit\n",
      "{'loss': 0.1199, 'learning_rate': 7.87878787878788e-06, 'epoch': 6.06}          \n",
      "{'loss': 0.1402, 'learning_rate': 7.272727272727273e-06, 'epoch': 6.36}         \n",
      "{'loss': 0.1496, 'learning_rate': 6.666666666666667e-06, 'epoch': 6.67}         \n",
      "{'loss': 0.1043, 'learning_rate': 6.060606060606061e-06, 'epoch': 6.97}         \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 231/330 [00:55<00:16,  6.17it/s][INFO|trainer.py:2777] 2023-09-07 05:14:13,212 >> Saving model checkpoint to output/checkpoint-231\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:13,214 >> Configuration saved in output/checkpoint-231/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:13,740 >> Model weights saved in output/checkpoint-231/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:13,740 >> Image processor saved in output/checkpoint-231/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:14,801 >> Deleting older checkpoint [output/checkpoint-198] due to args.save_total_limit\n",
      "{'loss': 0.1043, 'learning_rate': 5.4545454545454545e-06, 'epoch': 7.27}        \n",
      "{'loss': 0.1445, 'learning_rate': 4.848484848484849e-06, 'epoch': 7.58}         \n",
      "{'loss': 0.107, 'learning_rate': 4.242424242424243e-06, 'epoch': 7.88}          \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 264/330 [01:02<00:08,  7.54it/s][INFO|trainer.py:2777] 2023-09-07 05:14:19,648 >> Saving model checkpoint to output/checkpoint-264\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:19,650 >> Configuration saved in output/checkpoint-264/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:20,191 >> Model weights saved in output/checkpoint-264/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:20,191 >> Image processor saved in output/checkpoint-264/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:21,259 >> Deleting older checkpoint [output/checkpoint-231] due to args.save_total_limit\n",
      "{'loss': 0.0822, 'learning_rate': 3.6363636363636366e-06, 'epoch': 8.18}        \n",
      "{'loss': 0.1258, 'learning_rate': 3.0303030303030305e-06, 'epoch': 8.48}        \n",
      "{'loss': 0.0967, 'learning_rate': 2.4242424242424244e-06, 'epoch': 8.79}        \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 297/330 [01:08<00:04,  7.85it/s][INFO|trainer.py:2777] 2023-09-07 05:14:26,115 >> Saving model checkpoint to output/checkpoint-297\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:26,116 >> Configuration saved in output/checkpoint-297/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:26,696 >> Model weights saved in output/checkpoint-297/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:26,697 >> Image processor saved in output/checkpoint-297/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:27,817 >> Deleting older checkpoint [output/checkpoint-264] due to args.save_total_limit\n",
      "{'loss': 0.1195, 'learning_rate': 1.8181818181818183e-06, 'epoch': 9.09}        \n",
      "{'loss': 0.1012, 'learning_rate': 1.2121212121212122e-06, 'epoch': 9.39}        \n",
      "{'loss': 0.1148, 'learning_rate': 6.060606060606061e-07, 'epoch': 9.7}          \n",
      "{'loss': 0.1236, 'learning_rate': 0.0, 'epoch': 10.0}                           \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330/330 [01:15<00:00,  7.99it/s][INFO|trainer.py:2777] 2023-09-07 05:14:32,690 >> Saving model checkpoint to output/checkpoint-330\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:32,691 >> Configuration saved in output/checkpoint-330/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:33,234 >> Model weights saved in output/checkpoint-330/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:33,234 >> Image processor saved in output/checkpoint-330/preprocessor_config.json\n",
      "[INFO|trainer.py:2894] 2023-09-07 05:14:34,291 >> Deleting older checkpoint [output/checkpoint-297] due to args.save_total_limit\n",
      "[INFO|trainer.py:1934] 2023-09-07 05:14:34,367 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 77.0132, 'train_samples_per_second': 68.56, 'train_steps_per_second': 4.285, 'train_loss': 0.2523378314393939, 'epoch': 10.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 330/330 [01:17<00:00,  4.29it/s]\n",
      "[INFO|trainer.py:2777] 2023-09-07 05:14:34,371 >> Saving model checkpoint to output\n",
      "[INFO|configuration_utils.py:458] 2023-09-07 05:14:34,372 >> Configuration saved in output/config.json\n",
      "[INFO|modeling_utils.py:1851] 2023-09-07 05:14:36,487 >> Model weights saved in output/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:234] 2023-09-07 05:14:36,488 >> Image processor saved in output/preprocessor_config.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  train_loss               =     0.2523\n",
      "  train_runtime            = 0:01:17.01\n",
      "  train_samples_per_second =      68.56\n",
      "  train_steps_per_second   =      4.285\n",
      "[INFO|trainer.py:3081] 2023-09-07 05:14:36,489 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3083] 2023-09-07 05:14:36,489 >>   Num examples = 80\n",
      "[INFO|trainer.py:3086] 2023-09-07 05:14:36,489 >>   Batch size = 16\n",
      "2023-09-07 05:14:37.000080: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:37.000139: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_2854566206946126162+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:37.000877: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:37.000878: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_4411201967751710878+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:37.000878: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:37.000879: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_16407023877168380706+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000038: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000039: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_10989985540458594900+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000042: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000043: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_12751598928191034930+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000226: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000226: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000227: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_6439607758445774205+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000227: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_8598648585105326671+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000381: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000382: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_6571166010089848396+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000382: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000383: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_12991406247443270585+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]2023-09-07 05:14:38.000619: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000619: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000619: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_11120858686510186620+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000620: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_2506019054446791935+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000806: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000806: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000807: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_11401904622186785880+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:38.000807: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_11228636114469661807+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 2/5 [00:00<00:00,  4.73it/s]2023-09-07 05:14:38.000990: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:38.000991: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_15664435241768717754+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000193: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000194: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_5858131689908442359+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000195: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000196: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_9695776144602196295+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000381: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000382: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_14997005377111877814+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000383: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000384: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_11633199607740551230+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 3/5 [00:00<00:00,  2.75it/s]2023-09-07 05:14:39.000568: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000569: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_10728869769052769180+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000771: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000772: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_12967210791014275590+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000774: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000775: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_1531972595600023427+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000961: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000962: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_16526598599832905732+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:39.000964: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:39.000965: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_16704114537050172472+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 4/5 [00:01<00:00,  2.25it/s]2023-09-07 05:14:40.000149: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000150: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_4346820354563907805+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:40.000352: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000353: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_14015629166471374106+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:40.000356: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000357: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_14365343826444546122+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:40.000545: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000546: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000547: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_14102846367852446252+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "2023-09-07 05:14:40.000547: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_8765945782161052574+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.02it/s]2023-09-07 05:14:40.000736: INFO ||NCC_WRAPPER||: Compile cache path: ./compiler_cache\n",
      "2023-09-07 05:14:40.000737: INFO ||NCC_WRAPPER||: Using a cached neff at ./compiler_cache/neuronxcc-2.8.0.25+a3ad0f342/MODULE_1620854833423187426+d41d8cd9/model.neff. Exiting with a successfully compiled graph.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.07it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_accuracy           =     0.9875\n",
      "  eval_loss               =     0.1033\n",
      "  eval_runtime            = 0:00:04.33\n",
      "  eval_samples_per_second =      18.47\n",
      "  eval_steps_per_second   =      0.693\n",
      "CPU times: user 1.24 s, sys: 307 ms, total: 1.55 s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!XLA_USE_BF16=1 NEURON_CC_FLAGS=\"--cache_dir=./compiler_cache\" \\\n",
    "torchrun --nproc_per_node=2 run_image_classification.py \\\n",
    "--model_name_or_path \"google/vit-base-patch16-224-in21k\" \\\n",
    "--dataset_name \"beans\" \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--num_train_epochs 10 \\\n",
    "--per_device_train_batch_size 16 \\\n",
    "--per_device_eval_batch_size 16 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--logging_strategy steps \\\n",
    "--logging_steps 10 \\\n",
    "--save_strategy epoch \\\n",
    "--save_total_limit 1 \\\n",
    "--seed 1337 \\\n",
    "--remove_unused_columns False \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5101401-df06-4a6d-9780-8832b9f28e01",
   "metadata": {},
   "source": [
    "ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã‚’å«ã‚“ã å­¦ç¿’ã«ã¯ `inf2.8xlarge` ä¸Šã§å®Ÿè¡Œã—ãŸå ´åˆã§ 25\\~30åˆ†ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™.\n",
    "2åº¦ç›®ä»¥é™ã®å®Ÿè¡Œã§ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆ©ç”¨å¯èƒ½ãªãŸã‚ã€1\\~2åˆ†ç¨‹åº¦ã§å­¦ç¿’ãŒå®Œäº†ã—ã¾ã™ã€‚\n",
    "\n",
    "`neuron_parallel_compile` ã‚³ãƒãƒ³ãƒ‰ã‚’åˆ©ç”¨ã—ãŸã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã®å‰Šæ¸›æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«](https://github.com/AWShtokoyo/aws-ml-jp/tree/main/frameworks/aws-neuron-jp/bertj_finetuning_classification)ã‚’å‚ç…§ä¸‹ã•ã„ã€‚\n",
    "\n",
    "\n",
    "ã“ã‚Œã§ã€€AWS Inferentia2 ä¸Šã§ã® ViT ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«æˆåŠŸã—ã¾ã—ãŸã€‚ \n",
    "`pytorch_model.bin` ã¨ã„ã†åå‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸé‡ã¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã€`Trainer` ã®çŠ¶æ…‹ã€ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`config.json`ï¼‰ ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a484cc-a790-4544-ae04-793ea288c16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 335268\n",
      "-rw-rw-r-- 1 ubuntu ubuntu      1614 Sep  7 05:14 README.md\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       329 Sep  7 05:14 all_results.json\n",
      "drwxrwxr-x 2 ubuntu ubuntu      4096 Sep  7 05:14 checkpoint-330\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       845 Sep  7 05:14 config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       185 Sep  7 05:14 eval_results.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       325 Sep  7 05:14 preprocessor_config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 343269037 Sep  7 05:14 pytorch_model.bin\n",
      "-rw-rw-r-- 1 ubuntu ubuntu       165 Sep  7 05:14 train_results.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu      4548 Sep  7 05:14 trainer_state.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu      3963 Sep  7 05:14 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27169e-de1c-438e-b095-089dca381925",
   "metadata": {},
   "source": [
    "# ViT æ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba773524-55dd-4999-a42d-41deec2e9077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model from provided checkpoint: ./output/\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "# Create the feature extractor and model\n",
    "checkpoint_dir = './output/'\n",
    "print(f\"Create model from provided checkpoint: {checkpoint_dir}\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(checkpoint_dir)\n",
    "model = ViTForImageClassification.from_pretrained(checkpoint_dir, torchscript=True)\n",
    "model.eval()\n",
    "\n",
    "# Get an example input\n",
    "url = \"https://datasets-server.huggingface.co/assets/beans/--/default/test/0/image/image.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "example = (inputs['pixel_values'],)\n",
    "\n",
    "# Run inference on CPU\n",
    "output_cpu = model(*example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6950e6-7e4d-40a5-84d0-fd65e6265f15",
   "metadata": {},
   "source": [
    "## æ¨è«–å®Ÿè¡Œã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
    "\n",
    "æ¨è«–ã‚’ AWS Inferentia2ã€€ã¾ãŸã¯ AWS Trainium ä¸Šã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’`torch_neuronx.trace` APIã‚’ç”¨ã„ã¦äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰ã—ãŸçµæœã¯ä¿å­˜ã™ã‚‹ã“ã¨ã§å†åˆ©ç”¨å¯èƒ½ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936134c6-12bb-426f-9316-1f6b9e09bbad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model for neuron with torch tracing ...\n",
      "Save compiled model as: vit-model-neuron.pt\n",
      "CPU times: user 7.65 s, sys: 2.31 s, total: 9.96 s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compile the model for neuron\n",
    "print(f\"Compile model for neuron with torch tracing ...\")\n",
    "model_neuron = torch_neuronx.trace(model, example)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'vit-model-neuron.pt'\n",
    "torch.jit.save(model_neuron, filename)\n",
    "print(f\"Save compiled model as: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac19f68-ebe6-464a-937f-1bbcb9b9b076",
   "metadata": {},
   "source": [
    "æœŸå¾…é€šã‚Šã®å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹ã‹ã©ã†ã‹ã€€CPUä¸Šã§ã®æ¨è«–çµæœã¨æ¯”è¼ƒã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8658aef-2c36-4d1d-bdd5-13bb38db0be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TorchScript compiled model\n",
    "print(f\"Load compiled model: {filename}\")\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Run inference using the Neuron model\n",
    "print(f\"Run inference on the test image: {url}\")\n",
    "output_neuron = model_neuron(*example)\n",
    "\n",
    "# Compare the results\n",
    "print(f\"--- Compare Neuron output against CPU output ----\")\n",
    "print(f\"CPU tensor:            {output_cpu[0][0][0:10]}\")\n",
    "print(f\"Neuron tensor:         {output_neuron[0][0][0:10]}\")\n",
    "print(f\"CPU prediction:    {model.config.id2label[output_cpu[0].argmax(-1).item()]}\")\n",
    "print(f\"Neuron prediction: {model.config.id2label[output_neuron[0].argmax(-1).item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfaae7-da47-458b-b6fa-023e2e5a06f8",
   "metadata": {},
   "source": [
    "## Gradio API ã‚’ç”¨ã„ãŸæ¨è«–ãƒ‡ãƒ¢\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‡ãƒ¢ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ç°¡æ˜“ãªæ–¹æ³•ã¯ã€Gradio API ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã€æ¨è«–çµæœã‚’ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a024e5-b244-488f-9bee-7d7b74a59f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "id2label = {0: 'angular_leaf_spotã€€(è§’è‘‰ã‚¹ãƒãƒƒãƒˆ)', 1: 'bean_rustã€€(è±†ã•ã³ç—…)', 2: 'healthyã€€(å¥åº·)'}\n",
    "\n",
    "def predict(raw_image):\n",
    "    size = (224, 224)\n",
    "    image_mean = [0.5, 0.5, 0.5]\n",
    "    image_std = [0.5, 0.5, 0.5]\n",
    "    normalize = Normalize(mean=image_mean, std=image_std)\n",
    "    \n",
    "    _val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    transformed_image = _val_transforms(raw_image.convert(\"RGB\"))\n",
    "    batched_transformed_image = transformed_image.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model_neuron(batched_transformed_image)\n",
    "        pred = id2label[prediction[0].argmax(-1).item()]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1f39d-d001-46d1-a2cf-a19edcad9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(fn=predict,\n",
    "             inputs=gr.Image(type=\"pil\"),\n",
    "             outputs=\"text\",\n",
    "             examples=[\n",
    "                 'image_samples/healthy_test.21.jpg',\n",
    "                 'image_samples/angular_leaf_spot_test.21.jpg',\n",
    "                 'image_samples/bean_rust_test.34.jpg'])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
