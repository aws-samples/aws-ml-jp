{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c348b8",
   "metadata": {},
   "source": [
    "# bilingual-gpt-neox-4b-instruction-sft を SageMaker で Hosting\n",
    "## このノートブックについて\n",
    "このノートブックは、rinna の bilingual-gpt-neox-4b-instruction-sft モデルを、SageMaker でリアルタイム推論エンドポイントを Hosting するノートブックです。  \n",
    "以下の環境で動作確認を行ってます。\n",
    "* SageMaker Studio Notebooks\n",
    "    * `ml.g5.2xlarge(NVIDIA A10G Tensor Core GPU 搭載 VRAM 24GB, RAM 32GB, vCPU 8)` : `PyTorch 1.13 Python 3.9 GPU Optimized`\n",
    "    * `ml.m5.2xlarge(RAM 32GB, vCPU 8) ` : `PyTorch 1.13 Python 3.9 CPU Optimized`\n",
    "* SageMaker Notebooks\n",
    "    * `ml.g5.2xlarge(NVIDIA A10G Tensor Core GPU 搭載 VRAM 24GB, RAM 32GB, vCPU 8)` : `conda_pytorch_p39`\n",
    "    * `ml.m5.2xlarge(RAM 32GB, vCPU 8) ` : `conda_pytorch_p39`  \n",
    "[各インスタンスの料金についてはこちら](https://aws.amazon.com/jp/sagemaker/pricing/)をご確認ください。  \n",
    "\n",
    "## 使用するモデルについて\n",
    "モデルの詳細については[Hugging Face bilingual-gpt-neox-4b-instruction-sft](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-sft)  を参照してください。  \n",
    "モデルのライセンスは上記リンクにあるとおり `MIT` です。\n",
    "\n",
    "ノートブックは外部ファイルを参照していないので、どのディレクトリに配置してあっても動作します。  \n",
    "\n",
    "また、ノートブックを動かすにあたって、各セルを上から順番に実行すれば動きますが、SageMaker 上での推論の仕組みについては、[AI/ML DarkPark](https://www.youtube.com/playlist?list=PLAOq15s3RbuL32mYUphPDoeWKUiEUhcug) の特に [Amazon SageMaker 推論 Part2すぐにプロダクション利用できる！モデルをデプロイして推論する方法 【ML-Dark-04】【AWS Black Belt】](https://youtu.be/sngNd79GpmE) をご参照ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2fddb-de17-4ce9-b622-227d06290e17",
   "metadata": {},
   "source": [
    "## 準備\n",
    "### ノートブックを動かすに当たって必要なモジュールのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0436c68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.26\n",
      "  Using cached transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting einops\n",
      "  Using cached einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.9/site-packages (2.132.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.173.0.tar.gz (854 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m854.4/854.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting SentencePiece\n",
      "  Using cached sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (4.64.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (23.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting attrs<24,>=23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting boto3<2.0,>=1.26.131\n",
      "  Downloading boto3-1.28.15-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.9/738.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jsonschema\n",
      "  Downloading jsonschema-4.18.4-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting platformdirs\n",
      "  Downloading platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Collecting tblib==1.7.0\n",
      "  Using cached tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.0)\n",
      "Collecting botocore<1.32.0,>=1.31.15\n",
      "  Downloading botocore-1.31.15-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26) (2023.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.13.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting referencing>=0.28.4\n",
      "  Downloading referencing-0.30.0-py3-none-any.whl (25 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26) (3.4)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.173.0-py2.py3-none-any.whl size=1163294 sha256=3aa30681b12970d597bf08a488f0468d28ead26bae6644ab25625778c8a150af\n",
      "  Stored in directory: /root/.cache/pip/wheels/91/c8/85/9cca0f426f8add1ff3f3bd74cacdb348382fd56d905b8c1c37\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: tokenizers, SentencePiece, tblib, rpds-py, regex, pyyaml, platformdirs, filelock, einops, attrs, referencing, huggingface-hub, botocore, transformers, jsonschema-specifications, jsonschema, boto3, sagemaker\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.2.0\n",
      "    Uninstalling attrs-22.2.0:\n",
      "      Successfully uninstalled attrs-22.2.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.70\n",
      "    Uninstalling botocore-1.29.70:\n",
      "      Successfully uninstalled botocore-1.29.70\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.70\n",
      "    Uninstalling boto3-1.26.70:\n",
      "      Successfully uninstalled boto3-1.26.70\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.132.0\n",
      "    Uninstalling sagemaker-2.132.0:\n",
      "      Successfully uninstalled sagemaker-2.132.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.70 requires botocore==1.29.70, but you have botocore 1.31.15 which is incompatible.\n",
      "awscli 1.27.70 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SentencePiece-0.1.99 attrs-23.1.0 boto3-1.28.15 botocore-1.31.15 einops-0.6.1 filelock-3.12.2 huggingface-hub-0.16.4 jsonschema-4.18.4 jsonschema-specifications-2023.7.1 platformdirs-3.10.0 pyyaml-6.0.1 referencing-0.30.0 regex-2023.6.3 rpds-py-0.9.2 sagemaker-2.173.0 tblib-1.7.0 tokenizers-0.13.3 transformers-4.26.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.26 einops sagemaker SentencePiece -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcddf57-19a5-455e-95e7-25703783a2a0",
   "metadata": {},
   "source": [
    "###  今回扱うモデルの動かし方について\n",
    "[How to use the model](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-sft#how-to-use-the-model) に沿って実行すると動かせます。  \n",
    "例えば、以下のコードをこのノートブックで実行するとテキストを生成できます。  \n",
    "実行したい場合は別途セルを用意して実行してみてください。g5.2xlarge インスタンスで実行に 10 分程度かかります。(ほとんどはモデルのロード時間です)  \n",
    "このノートブックでは以下のコードをベースに SageMaker で Hosting できるようにします。  \n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'rinna/bilingual-gpt-neox-4b-instruction-sft', \n",
    "    use_fast=False\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'rinna/bilingual-gpt-neox-4b-instruction-sft'\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = '''ユーザー: 世界自然遺産を列挙してください。\n",
    "システム: 膨大な数です。例えば国で絞ってください。\n",
    "ユーザー: イギリスでお願いします。\n",
    "システム:'''.replace('\\n','<NL>')\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        do_sample=True,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.01,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "output = output.replace(\"<NL>\", \"\\n\")\n",
    "print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31ac1d-e300-44a3-8d38-f709e7370ad8",
   "metadata": {},
   "source": [
    "### モジュール読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636d9a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "region = boto3.session.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sm = boto3.client('sagemaker')\n",
    "smr = boto3.client('sagemaker-runtime')\n",
    "endpoint_inservice_waiter = sm.get_waiter('endpoint_in_service')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fe9a8",
   "metadata": {},
   "source": [
    "### モデルのダウンロード\n",
    "SageMaker で機械学習モデルをホスティングする際は、一般的にはモデルや推論コードなどを tar.gz の形に固めます。  \n",
    "tokenizer と model を `from_pretrained` メソッドを利用してモデルをインターネットからロードして、そのままファイルをディレクトリに出力します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c6a8fb-28b7-414d-a00f-e70256021df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 既存のディレクトリがある場合のときのため削除\n",
    "model_dir = './inference'\n",
    "!rm -rf {model_dir}\n",
    "!mkdir -p {model_dir}'/code'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0b8e0-33e5-4482-90bc-2a47eb4290ae",
   "metadata": {},
   "source": [
    "#### tokenizer の取得と保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab583c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 284/284 [00:00<00:00, 43.5kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 1.34M/1.34M [00:00<00:00, 79.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 223 ms, sys: 23.6 ms, total: 246 ms\n",
      "Wall time: 775 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./inference/tokenizer_config.json',\n",
       " './inference/special_tokens_map.json',\n",
       " './inference/spiece.model',\n",
       " './inference/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'rinna/bilingual-gpt-neox-4b-instruction-sft', \n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afb4a9-2812-4c3a-969d-fe8550b6812e",
   "metadata": {},
   "source": [
    "#### モデルの取得と保存\n",
    "以下のセルは 10GB 以上のモデルを DL して保存するため 5 分ほど時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e4a1bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 617/617 [00:00<00:00, 110kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 7.59G/7.59G [01:38<00:00, 76.7MB/s]\n",
      "Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at rinna/bilingual-gpt-neox-4b-instruction-sft and are newly initialized: ['gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.27.attention.bias', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.35.attention.bias', 'gpt_neox.layers.24.attention.masked_bias', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.33.attention.masked_bias', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.29.attention.bias', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.28.attention.bias', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.32.attention.bias', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.25.attention.bias', 'gpt_neox.layers.31.attention.masked_bias', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.30.attention.bias', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.32.attention.masked_bias', 'gpt_neox.layers.26.attention.masked_bias', 'gpt_neox.layers.30.attention.masked_bias', 'gpt_neox.layers.24.attention.bias', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.27.attention.masked_bias', 'gpt_neox.layers.19.attention.bias', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.25.attention.masked_bias', 'gpt_neox.layers.31.attention.bias', 'gpt_neox.layers.13.attention.masked_bias', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.34.attention.bias', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.34.attention.masked_bias', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.29.attention.masked_bias', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.35.attention.masked_bias', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.28.attention.masked_bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.26.attention.bias', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.33.attention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.9 s, sys: 28.4 s, total: 1min 12s\n",
      "Wall time: 5min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'rinna/bilingual-gpt-neox-4b-instruction-sft'\n",
    ")\n",
    "model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f84c0-2f23-4350-ab31-8844ebca0add",
   "metadata": {},
   "source": [
    "モデルは SageMaker で動かすのでメモリから開放します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce465780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed6305",
   "metadata": {},
   "source": [
    "### 推論コードの作成\n",
    "先程実行したコードをもとに記述していきます。  \n",
    "まずは必要なモジュールを記述した requirements.txt を用意します。  \n",
    "今回は [deep-learning-containers](https://github.com/aws/deep-learning-containers)の HuggingFace のコンテナを使います。  \n",
    "einops と Sentence Piece が不足しているので requirements.txt に記載します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26fdc6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference/code/requirements.txt\n",
    "einops\n",
    "SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae3d83-b4c9-4b2b-a1d5-9fba9a35c93b",
   "metadata": {},
   "source": [
    "先述のコードを SageMaker Inference 向けに改変します。\n",
    "1. `model_fn` でモデルを読み込みます。先程は huggingface のモデルを直接ロードしましたが、`model_dir` に展開されたモデルを読み込みます。\n",
    "2. `input_fn` で前処理を行います。\n",
    "    * json 形式のみを受け付け他の形式は弾くようにします。\n",
    "    * json 文字列を dict 形式に変換して返します。\n",
    "3. `predict_fn` で推論します。\n",
    "    1. リクエストされたテキストを token 化します。\n",
    "    2. パラメータを展開します。\n",
    "    3. 推論（生成）します。\n",
    "    4. 生成結果をテキストにして返します。\n",
    "4. `output_fn` で結果を json 形式にして返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fbc4cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference/code/inference.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir, \n",
    "        use_fast=False\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir\n",
    "    ).to(DEVICE)\n",
    "    return {'tokenizer':tokenizer,'model':model}\n",
    "\n",
    "def input_fn(data, content_type):\n",
    "    if content_type == 'application/json':\n",
    "        data = json.loads(data)\n",
    "    else:\n",
    "        raise TypeError('content_type is only allowed application/json')\n",
    "    return data\n",
    "\n",
    "def predict_fn(data, model):\n",
    "    prompt = data['prompt']\n",
    "    token_ids = model['tokenizer'].encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    do_sample = data['do_sample']\n",
    "    max_new_tokens = data['max_new_tokens']\n",
    "    temperature = data['temperature']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model['model'].generate(\n",
    "            token_ids.to(DEVICE),\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=model['tokenizer'].pad_token_id,\n",
    "            bos_token_id=model['tokenizer'].bos_token_id,\n",
    "            eos_token_id=model['tokenizer'].eos_token_id\n",
    "        )\n",
    "    output = model['tokenizer'].decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "    output = output.replace(\"<NL>\", \"\\n\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def output_fn(data, accept_type):\n",
    "    if accept_type == 'application/json':\n",
    "        data = json.dumps({'result' : data})\n",
    "    else:\n",
    "        raise TypeError('content_type is only allowed application/json')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1a2d0",
   "metadata": {},
   "source": [
    "### モデルアーティファクトの作成と S3 アップロード\n",
    "アーティファクト(推論コード + モデル)を tar.gz に固めます。時間がかかるので `pigz` で並列処理を行います。  \n",
    "ml.g5.2xlarge, ml.m5.2xlarge で 10 分ほどかかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89a3a6-804d-4512-81f5-64a0bbfee47c",
   "metadata": {
    "tags": []
   },
   "source": [
    "※ SageMaker Studio のカーネルには pigz が入っていないので、下記 apt のセルを実行してください。SageMaker Notebooks の場合は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1200c53-ff5d-4b12-b7e5-8303a22f0680",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]                \u001b[0m\u001b[33m\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2597 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1085 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.3 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2909 kB][33m\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2738 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1383 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.0 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3389 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 27.7 MB in 2s (12.3 MB/s)33m                         \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "93 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  pigz\n",
      "0 upgraded, 1 newly installed, 0 to remove and 93 not upgraded.\n",
      "Need to get 57.4 kB of archives.\n",
      "After this operation, 259 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
      "Fetched 57.4 kB in 0s (984 kB/s)\u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package pigz.\n",
      "(Reading database ... 40690 files and directories currently installed.)\n",
      "Preparing to unpack .../archives/pigz_2.4-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking pigz (2.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up pigz (2.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt update -y\n",
    "!apt install pigz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b003b0fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/aws-ml-jp/tasks/generative-ai/text-to-text/inference/deploy-endpoint/Transformers/inference\n",
      "./\n",
      "./pytorch_model-00002-of-00002.bin\n",
      "./spiece.model\n",
      "./tokenizer_config.json\n",
      "./config.json\n",
      "./pytorch_model.bin.index.json\n",
      "./generation_config.json\n",
      "./special_tokens_map.json\n",
      "./code/\n",
      "./code/requirements.txt\n",
      "./code/inference.py\n",
      "./pytorch_model-00001-of-00002.bin\n",
      "/root/aws-ml-jp/tasks/generative-ai/text-to-text/inference/deploy-endpoint/Transformers\n",
      "CPU times: user 8.59 s, sys: 950 ms, total: 9.54 s\n",
      "Wall time: 10min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!rm model.tar.gz\n",
    "%cd {model_dir}\n",
    "!tar  cv ./ | pigz -p 8 > ../model.tar.gz # 8 並列でアーカイブ\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f9124-a8d9-4460-a7f5-c350b8d05629",
   "metadata": {},
   "source": [
    "アーティファクトを S3 にアップロードします。60 秒程度で完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e9a3a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-290000338583/bilingual-gpt-neox-4b-instruction-sft/model.tar.gz\n",
      "CPU times: user 45.4 s, sys: 44.5 s, total: 1min 29s\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_s3_uri = sagemaker.session.Session().upload_data(\n",
    "    'model.tar.gz',\n",
    "    key_prefix='bilingual-gpt-neox-4b-instruction-sft'\n",
    ")\n",
    "print(model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67269333",
   "metadata": {},
   "source": [
    "## SageMaker で Hosting する\n",
    "g5.2xlarge インスタンス(NVIDIA A10G Tensor Core GPU 搭載 VRAM 24GB, RAM 32GB) の場合レスポンスに 6 秒程度で済むため、リアルタイム推論エンドポイントを立てます。  \n",
    "(再掲)g5.2xlarge の[料金はこちら](https://aws.amazon.com/sagemaker/pricing/?nc1=h_ls)で確認してください。  \n",
    "\n",
    "リアルタイム推論エンドポイントを立てて推論するにあたって、SageMaker Python SDK を用いる場合と Boto3 を用いる場合の 2 パターンを紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d0ab5",
   "metadata": {},
   "source": [
    "### SageMaker Python SDKを用いる場合\n",
    "\n",
    "#### Hosting\n",
    "使用している API の詳細は以下を確認してください。  \n",
    "[Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8e337-19bc-414e-9828-44fe06cd6854",
   "metadata": {},
   "source": [
    "##### 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfe392f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'bilingual-gpt-neox-4b-instruction-sft'\n",
    "endpoint_config_name = model_name + 'Config'\n",
    "endpoint_name = model_name + 'Endpoint'\n",
    "instance_type = 'ml.g5.2xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e9517-82f5-4c53-8015-725397aad119",
   "metadata": {},
   "source": [
    "##### 使用するコンテナイメージの URI を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "440e5c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='huggingface',\n",
    "    region=region,\n",
    "    version='4.26',\n",
    "    image_scope='inference',\n",
    "    base_framework_version='pytorch1.13',\n",
    "    instance_type = instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102d46c-e265-411c-89e0-1cf7b8071b85",
   "metadata": {},
   "source": [
    "##### モデルの定義\n",
    "先程 S3 にアップロードしたアーティファクトの tar.gz の URI と、コンテナイメージの URI, ロールを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68d03094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data = model_s3_uri,\n",
    "    role = role,\n",
    "    image_uri = image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960d528-946b-4efa-9b46-6197c5544c8e",
   "metadata": {},
   "source": [
    "デプロイ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a98cf65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fccf8b",
   "metadata": {},
   "source": [
    "#### 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1f0eb-b1a9-4512-b8ad-edcb5b22f655",
   "metadata": {},
   "source": [
    "##### promptについて\n",
    "[japanese-gpt-neox-3.6b-instruction-ppo#io-format](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo#io-format) にある通り、以下の通りにすると良い結果が得られやすいです。  \n",
    "\n",
    "* プロンプトはユーザーとシステムの会話形式で与える\n",
    "* 各発言は、以下形式に則る  \n",
    "    `{ユーザー, システム} : {発言}`\n",
    "* プロンプトの末尾は`システム:` で終了させる\n",
    "* 改行は`<NL>`を利用し、発言はすべて `<NL>` で区切る必要がある\n",
    "\n",
    "以下はプロンプトの例です。`<NL>`の埋め込みが大変なので、改行で書いて後で置換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e3c1a93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ユーザー: 世界自然遺産を列挙してください。<NL>システム: 膨大な数です。例えば国で絞ってください。<NL>ユーザー: イギリスでお願いします。<NL>システム:\n"
     ]
    }
   ],
   "source": [
    "prompt = '''ユーザー: 世界自然遺産を列挙してください。\n",
    "システム: 膨大な数です。例えば国で絞ってください。\n",
    "ユーザー: イギリスでお願いします。\n",
    "システム:'''.replace('\\n','<NL>')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4551782e-9ec4-49e2-a398-80ffb5836f64",
   "metadata": {},
   "source": [
    "##### 推論リクエスト\n",
    "model_fn の実行に時間がかかってしまい、エンドポイントが IN_SERVICE になっても、初回推論はしばらく動かないことがあります。  \n",
    "CloudWatch Logs に以下のような表示がある場合はしばらく待てば使えるようになります。  \n",
    "`[WARN] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.`  \n",
    "モデルがロードされるまで 6 分程度かかるため、リトライを入れています。\n",
    "実際の推論時間は 6 秒程度です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e7e3b3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イギリスは世界自然遺産のリストに載っています。</s>\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "request = {\n",
    "    'prompt' : prompt,\n",
    "    'max_new_tokens' : 128,\n",
    "    'do_sample' : True,\n",
    "    'temperature' : 0.01,\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        output = predictor.predict(request)['result']\n",
    "        break\n",
    "    except:\n",
    "        sleep(60)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec071c-4b2f-4218-8705-9ea99606f516",
   "metadata": {},
   "source": [
    "##### エンドポイントの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f85c17da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896098a",
   "metadata": {},
   "source": [
    "### Boto3 を用いる場合\n",
    "標準だと SageMaker SDK が入っていない環境からデプロイや推論する場合(例:AWS Lambda など)は、boto3 でデプロイや推論することも多いです。  \n",
    "以下のセルは boto3 で実行する方法を記述しています。\n",
    "各 API の詳細は Document を確認してください。  \n",
    "[SageMaker](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html)  \n",
    "[SageMakerRuntime](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html)  \n",
    "\n",
    "#### Hosting\n",
    "##### モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6754ae19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': image_uri,\n",
    "        'ModelDataUrl': model_s3_uri,\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
    "            'SAGEMAKER_REGION': region,\n",
    "        }\n",
    "    },\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e4d68-56a0-440c-bce0-65c61f2f364d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### エンドポイントコンフィグの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a118dc38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTrafic',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.g5.2xlarge',\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d159b7-b6ca-401e-82f5-0682a597d162",
   "metadata": {},
   "source": [
    "##### エンドポイントの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e39686d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "endpoint_inservice_waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={'Delay': 5,}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017ad04-23c8-4259-83a6-4812b3784291",
   "metadata": {},
   "source": [
    "#### 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e6b059d-89d1-45c0-819a-e7f3b412e74a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'ユーザー: 世界自然遺産を列挙してください。<NL>システム: 膨大な数です。例えば国で絞ってください。<NL>ユーザー: イギリスでお願いします。<NL>システム:', 'max_new_tokens': 128, 'do_sample': True, 'temperature': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# prompt 確認\n",
    "print(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69159367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イギリスは世界自然遺産のリストに載っています。</s>\n",
      "CPU times: user 35.6 ms, sys: 0 ns, total: 35.6 ms\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 推論\n",
    "for i in range(10):\n",
    "    try:\n",
    "        response = smr.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Accept='application/json',\n",
    "            Body=json.dumps(request)\n",
    "        )\n",
    "        break\n",
    "    except:\n",
    "        sleep(60)\n",
    "output = json.loads(response['Body'].read().decode('utf-8'))['result']\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf1199f-f812-46f5-8b2d-423f0e9b6ece",
   "metadata": {},
   "source": [
    "#### お片付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b036b51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '57000c18-650e-4d77-af8f-41cd7ba4d182',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '57000c18-650e-4d77-af8f-41cd7ba4d182',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 31 Jul 2023 06:15:15 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d781e1-e2f1-41c8-83a6-59608b133c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
