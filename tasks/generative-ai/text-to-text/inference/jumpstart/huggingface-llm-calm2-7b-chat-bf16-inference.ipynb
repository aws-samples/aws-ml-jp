{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキスト生成: 構築したエンドポイントを使用して推論を実行する方法\n",
    "\n",
    "このサンプルノートブックでは、CyberAgentLM2 をデプロイし、複数のプロンプトエンジニアリングテクニックを利用し検証します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- [エンドポイントのデプロイ](#エンドポイントのデプロイ): [CyberAgentLM2-7B-Chat (CALM2)](https://huggingface.co/cyberagent/calm2-7b-chat) をエンドポイントにデプロイする\n",
    "- [推論の準備](#推論の準備): [CyberAgentLM2-7B-Chat (CALM2)](https://huggingface.co/cyberagent/calm2-7b-chat) を試すための import やユーティリティメソッドの実装です\n",
    "- [Zero-Shot を試す](#zero-shot-を試す): Zero-Shot によって、海賊風に回答させることに挑戦します\n",
    "- [Few-Shot プロンプトを試す](#few-shot-プロンプトを試す): Few-Shot プロンプトによって、センチメント分析に挑戦します\n",
    "- [質問応答を試す](#質問応答を試す): Amazon CEO 2022 年の書簡に対して、What, How を問う質問応答に挑戦します\n",
    "- [要約を試す](#要約を試す): Amazon CEO 2022 年の書簡の要約に挑戦します\n",
    "- [ChatBot を試す](#chatBot-を試す): 会話の履歴をプロンプトに入れることによって、コンテキストに沿った対話に挑戦します\n",
    "- [計算を試す](#計算を試す): 足算に挑戦します\n",
    "- [Agent を試す](#Agent-を試す): Agent を活用して足算の回答精度を高めることに挑戦します\n",
    "- [発展](#発展): こちらの NoteBook にて学び終えた次の一手について記載しています\n",
    "- [付録: テキスト生成実行時に渡せるパラメータの説明](#付録-テキスト生成実行時に渡せるパラメータの説明): テキスト生成実行時に渡せるパラメータを説明しています"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントのデプロイ\n",
    "\n",
    "SageMaker JumpStart から CyberAgentLM2 を選択し、エンドポイントをデプロイします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "\n",
    "dropdown = Dropdown(\n",
    "    options=list_jumpstart_models(\"search_keywords includes Text Generation\"),\n",
    "    value=\"huggingface-llm-calm2-7b-chat-bf16\",\n",
    "    description=\"Select a JumpStart text generation model:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = dropdown.value\n",
    "model_version = \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論の準備\n",
    "[CyberAgentLM2-7B-Chat (CALM2)](https://huggingface.co/cyberagent/calm2-7b-chat) を推論するための準備をしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CALM2](https://huggingface.co/cyberagent/calm2-7b-chat) では \"USER: xxx\\nASSISTANT: yyy\" というフォーマットでプロンプトを書くことができます。このフォーマットに合わせるためのユーティリティメソッドを実装しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt (prompt_pairs, system_add=True):\n",
    "    prompt = [\n",
    "        f\"{uttr['speaker']}: {uttr['text']}\" + (\"<|endoftext|>\" if uttr['speaker'] == \"ASSISTANT\" else \"\")\n",
    "        for uttr in prompt_pairs\n",
    "    ]\n",
    "    prompt = \"\\n\".join(prompt)\n",
    "    if system_add:\n",
    "        prompt = (\n",
    "            prompt\n",
    "            + \"\\n\"\n",
    "            + \"ASSISTANT: \"\n",
    "        )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論 endpoint にリクエストして、結果を表示するユーティリティメソッドを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "endpoint_name = predictor.endpoint_name\n",
    "\n",
    "def query_endpoint(payload, do_print = False):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/json', Body=json.dumps(payload).encode('utf-8'))\n",
    "    model_predictions = json.loads(response['Body'].read())\n",
    "    generated_text = model_predictions[0]['generated_text']\n",
    "    if do_print:\n",
    "        print (\n",
    "            f\"Input Text: {payload['inputs']}{newline}\"\n",
    "            f\"Generated Text: {bold}{generated_text}{unbold}{newline}\")\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでで準備は整いました。まずは、[CALM2](https://huggingface.co/cyberagent/calm2-7b-chat) に記載のサンプルプロンプトを試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 10,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"stop\": [\"<|endoftext|>\", \"USER:\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"Hello, you are an assistant that helps me learn Japanese.\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"ASSISTANT\",\n",
    "        \"text\": \"Sure, what can I do for you?\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"VRはなんですか。\"\n",
    "    }\n",
    "]\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot を試す\n",
    "Zero-Shot (質問と回答の例を直接的にプロンプトに指定しない方法) によって口調を変えられるか試してみましょう。ここでは海賊風な回答ができるようになるか試していきます。まずはシンプルに試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\":  \"USER\",\n",
    "        \"text\":  \"あなたは海賊です。海賊の口調で答えてください。日本の居酒屋に行ってビールを頼む時どんな頼み方をしますか？\"\n",
    "    }\n",
    "]\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果はどうでしょうか? 海賊味が足りないかもしれませんね。ここで、海賊としての振る舞いを事前にプロンプトに含んだ状態で試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\":  \"USER\",\n",
    "        \"text\":  \"あなたの職業は何ですか？\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\":  \"ASSISTANT\",\n",
    "        \"text\":  \"私の職業は海賊だ。\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\":  \"USER\",\n",
    "        \"text\":  \"海賊はどんなことをしますか？\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\":  \"ASSISTANT\",\n",
    "        \"text\":  \"海賊は荒くれ者の集まりさ。どんなにひどい嵐の中でも酒を飲んで歌うんだ。\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\":  \"USER\",\n",
    "        \"text\":  \"海賊はどんな話し方をするのですか？\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\":  \"ASSISTANT\",\n",
    "        \"text\":  \"丁寧な言葉は使わないね。\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\":  \"USER\",\n",
    "        \"text\":  \"日本の居酒屋に行ってビールを頼む時どんな頼み方をしますか？\"\n",
    "    },    \n",
    "]\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いかがでしょうか? 海賊風な回答が返ってきたでしょうか？ セルを実行する度に回答が変化するため何度か実行して結果を観察してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot プロンプトを試す\n",
    "先ほどの例は回答例を示さずに期待する回答を導く方法でした。ここでは、いくつかの回答例を示すことで期待する結果を導いてみましょう。以下の例は、センチメント分析の例です。文章に対してラベル (ポジティブ / ネガティブ / ニュートラル) を回答するようにプロンプトを構成してみます。\n",
    "まずは、Few-Shot しない場合を試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"この新しいミュージックビデオは信じられないほど素晴らしかった。この文章のラベル (ポジティブ / ネガティブ / ニュートラル) を返してください。\"\n",
    "    }\n",
    "]\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答はどうでしょうか？ 何度か試してみるとラベル以外の回答をすること気づくでしょう。続いて、Few-Shot プロンプトを試してみましょう。文章とラベルのペアを事前にプロンプトとして与えておくことで回答方法を指示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"携帯電話のバッテリーが切れるのは嫌です。\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"ASSISTANT\",\n",
    "        \"text\": \"ネガティブ\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"私の一日は:+1。\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"ASSISTANT\",\n",
    "        \"text\": \"ポジティブ\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"これが記事へのリンクです\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"ASSISTANT\",\n",
    "        \"text\": \"ニュートラル\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"この新しいミュージックビデオは信じられないほど素晴らしかった\"\n",
    "    }\n",
    "]\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-Shot を使う場合と使わない場合とで複数回実行してみてください。Few-Shot の方が期待する回答が多い傾向に気づくはずです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 質問応答を試す\n",
    "[2022年 Amazon CEO の書簡](https://www.aboutamazon.jp/news/company-news/ceo-andy-jassys-2022-letter-to-shareholders)の第一段落を使用して質問応答を試してみましょう。質問対象となる文章をプロンプトに含めておきます。その文章に対して質問し、正しく回答できるかを観察してみましょう。ここではいくつか質問を試してみたいと思います。簡易的ですがプロンプトを template 化しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def template_question_answering(question):\n",
    "    ret = [\n",
    "        {\n",
    "            \"speaker\": \"USER\",\n",
    "            \"text\": f\"\"\"\\\n",
    "当社が毎年、発行している株主の皆様への書簡をCEOとして執筆するのは2年目となりますが、\n",
    "私は今、机に向かいながらAmazonの将来に期待と高揚感を感じています。\n",
    "マクロ経済において2022年は、ここ数年間に記憶している中でもとりわけ困難な年でした。\n",
    "当社においてもいくつかの経営上の課題が浮上しましたが、\n",
    "（過去最高の成長を記録した新型コロナウイルス感染症のパンデミック期の前半からさらに）需要を拡大させることができました。\n",
    "お客様の体験（カスタマーエクスペリエンス）を短期的、長期的に意味のある形で向上させていくため、\n",
    "主要事業の変革を推進しました。\n",
    "そして、お客様、株主の皆様、社員にさらに貢献できる企業となっていくために必要な長期的投資を継続しつつ、\n",
    "投資判断や今後の発明の在り方や進め方についても重要な調整を行いました。\n",
    "\n",
    "上記の文章から答えてください。{question}\n",
    "    \"\"\"\n",
    "        }\n",
    "    ]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"CEO が 書簡を書くのは何回目でしょうか?\"\n",
    "prompt = template_question_answering(question)\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "皆さんも自分ならどう回答するかを想定しながら、その回答とモデルが生成した回答とを比較してみてください。複数回実行して回答の変化を観察してみましょう。\n",
    "次は、先ほどより少し難しい質問をしてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"お客様の体験を向上させるために何をしましたか?\"\n",
    "prompt = template_question_answering(question)\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、プロンプトに回答の参考になる文章を含めることで質問応答することが可能になります。このプロンプトエンジニアリングは [RAG](https://aws.amazon.com/jp/blogs/news/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/) と呼ばれる方式でも重要な考え方の一つです。このサンプルプロンプトにより、[CALM2](https://huggingface.co/cyberagent/calm2-7b-chat) モデルの RAG への有用性を感じることができるでしょう。question に様々な質問を入力して試してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要約を試す\n",
    "ここでは、要約を試してみましょう。要約は長い文章を短しつつも必要な情報を残すことが重要です。質問応答のテンプレートを再利用して、要約の結果を観察してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"上記の文章を要約してください。\"\n",
    "prompt = template_question_answering(question)\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数回実行して結果を観察してみましょう。皆さんが期待する要約ができているでしょうか確認してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot を試す\n",
    "ここでは、ChatBot としての回答を観察してみましょう。ChatBot に求められる機能として、会話のコンテキスト (文脈) を考慮することが挙げられます。\n",
    "プロンプトに会話のやり取りを追加しておくことで会話のコンテキストを保ちます。これを実現するための簡易なユーティリティメソッドを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextualPrompt:\n",
    "    def __init__(self):\n",
    "        self._contextual_prompt = \"\"\n",
    "    \n",
    "    @property\n",
    "    def contextual_prompt(self):\n",
    "        return self._contextual_prompt\n",
    "    \n",
    "    def add_context(self, prompt, need_nl = False):\n",
    "        if need_nl:\n",
    "            self._contextual_prompt += \"\\n\"\n",
    "        self._contextual_prompt += prompt\n",
    "    \n",
    "    def add_speaker_context(self, speaker, text, need_nl = False):\n",
    "        prompt = [\n",
    "            {\n",
    "                \"speaker\": speaker,\n",
    "                \"text\": text\n",
    "            }\n",
    "        ]\n",
    "        self.add_context(format_prompt(prompt), need_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"私はエンジニアです。あなたはエンジニアアシスタントとして設計にアドバイスしてください。\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"ASSISTANT\",\n",
    "        \"text\": \"承知しました。\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"AWS でシンプルなアプリケーションを作ろうとしています。データベースは何を使うのが良いでしょうか？なるべく運用したくありません。\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"用途によりますが、Amazon RDS (Relational Database Service) for MySQL, PostgreSQL, Oracle, SQL Server, Amazon Auroraを推奨します。これらのデータベースは、高性能でスケーラブルなパフォーマンスを提供し、AWSのグローバルインフラストラクチャを活用して可用性と災害復旧を実現します。また、AWS Management Consoleから簡単に操作できることから、運用コストを大幅に削減できます。\"\n",
    "    },\n",
    "]\n",
    "formated_prompt = format_prompt(prompt, False)\n",
    "chat_prompt = ContextualPrompt()\n",
    "chat_prompt.add_context(formated_prompt)\n",
    "print(chat_prompt.contextual_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、ChatBot のキャラクター設定が反映されているか確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"どのような運用が必要ですか？\"\n",
    "    }\n",
    "]\n",
    "formated_prompt = format_prompt(prompt, True)\n",
    "chat_prompt.add_context(formated_prompt, True)\n",
    "print(chat_prompt.contextual_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": chat_prompt.contextual_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "response_text = query_endpoint(payload)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過去の文脈を汲んで Amazon RDS の運用について答えていたら成功です。会話履歴を反映したチャットが可能なことが確認できました。\n",
    "それでは、会話してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算を試す\n",
    "生成系 AI は一般的に計算が苦手と言われています。[CALM2](https://huggingface.co/cyberagent/calm2-7b-chat) はどうか確認してみましょう。どのようなプロンプトを得意とするのか試して観察することによりノウハウとして得ることはビジネスユースにおいて重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\": \"USER\",\n",
    "        \"text\": \"3 + 5 =\"\n",
    "    }\n",
    "]\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": formated_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "query_endpoint(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を確認してみましょう。期待する結果が返ってきたでしょうか? 複数回実行して結果を観察してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent を試す\n",
    "[計算を試す](#計算を試す)では直接足算を LLM に指示しました。しかし、私たちもそうであるように、電卓を使った方が正確に計算できます。この考え方を実現する方法が Agent です。Agent には [ReAct](https://arxiv.org/abs/2210.03629) などさまざまな手法が提案されており、およそ以下のような要素の組合せで実現されます。\n",
    "\n",
    "- 質問: ユーザ入力\n",
    "- 考察 (Thought): 質問に対する LLM の出力であり、行動の理由となるもの\n",
    "- 行動 (Action): 考察に基づき、LLM が採用すべき行動 (例えば 電卓を打つ) \n",
    "- 観測 (Observation): 行動から得られた結果であり、完了しない場合に質問に含めて考察、行動を繰り返すことでコンテキストを追加する\n",
    "- 完了 (Finish): 最終的な結果が得られたか、得られなければ質問、考察、行動を繰り返す\n",
    "\n",
    "ここでは、簡易化して、質問、考察から行動が導けるのかを足算を例に挑戦してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、先ほどと同じ方法で 10 回試して結果を観察しましょう。どれくらい正しく計算できているでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"speaker\": \"ユーザー\",\n",
    "        \"text\": \"3 + 5 =\"\n",
    "    }\n",
    "]\n",
    "formated_prompt = format_prompt(prompt)\n",
    "print(formated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    payload = {\n",
    "        \"inputs\": formated_prompt,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    print(query_endpoint(payload))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、Agent のアイデアに沿った Few-Shot を利用してみましょう。まずは、Tool となる電卓の準備です。簡易的に足算を表す式を `eval` するだけの実装にしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dentaku(expression):\n",
    "    return eval(expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-Shot を用意します。質問、考察、行動という言葉が持つ意味に引っ張られる傾向がある場合は、それぞれを表すランダムな文字列を利用することで、改善できる場合があります。これも LLM の特性に合わせたプロンプトエンジニアリングの一つです。LLM の特性を理解するために様々なプロンプトを試してみることの重要性が理解できます。ここでは、そのままの単語を利用してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"質問\": \"質問\",\n",
    "    \"考察\": \"考察\",\n",
    "    \"行動\": \"行動\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dentaku_agent_template(question, arg):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"speaker\": \"USER\",\n",
    "            \"text\": f\"{d['質問']} {question}\"\n",
    "        },\n",
    "        {\n",
    "            \"speaker\": \"ASSISTANT\",\n",
    "            \"text\": f\"{d['考察']} {arg} これは数式です。数式は Dentaku を使うべきです。{d['行動']} Dentaku[{arg}]\"\n",
    "        },\n",
    "    ]\n",
    "    return format_prompt(prompt, False)\n",
    "\n",
    "agent_prompt = ContextualPrompt()\n",
    "agent_prompt.add_context(dentaku_agent_template(\"1 + 1 =?\", \"1+1\"))\n",
    "agent_prompt.add_context(dentaku_agent_template(\"2 x 2 =?\", \"2*2\"), True)\n",
    "agent_prompt.add_context(dentaku_agent_template(\"4 / 2 =?\", \"4/2\"), True)\n",
    "agent_prompt.add_context(dentaku_agent_template(\"3 - 2 =?\", \"3-2\"), True)\n",
    "\n",
    "print(agent_prompt.contextual_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四則演算の Few-Shot ができました。それでは最後に求めたい計算式を入力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"3 + 5 =?\"\n",
    "agent_prompt.add_speaker_context(\"USER\", f\"{d['質問']} {question}\", True)\n",
    "print(agent_prompt.contextual_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "準備ができました。正しい考察と行動が導けるか確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": agent_prompt.contextual_prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "response_text = query_endpoint(payload)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正しい行動が導けたでしょうか？ 導けたなら、残りは行動に合わせて Tool を実行するハンドラーを実装するだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def handler(response_tex):\n",
    "    try:\n",
    "        target_string = \"Dentaku\"\n",
    "        if target_string in response_tex:\n",
    "            pattern = r'\\[(.*?)\\]'\n",
    "            matches = re.findall(pattern, response_tex)\n",
    "            if matches:\n",
    "                return dentaku(matches[0])\n",
    "            else:\n",
    "                print(\"Dentaku を使用しますが。計算式がわかりません。\")\n",
    "        else:\n",
    "            print(\"適切な Tool がありません。\")\n",
    "    except Exception as e: \n",
    "        print(f\"エラーが発生しました。判断できません。{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "handler(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの処理を複数回実行しやすいように 1 つのセルにまとめて実行してみましょう。LLM に直接足算を解かせる方法に比べて正しい回答が導けるようになったでしょうか? Tool として正しく Dentaku が選択されるでしょうか? また、計算は正しいでしょうか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def agent(question):\n",
    "    # Few-Shot の生成\n",
    "    agent_prompt = ContextualPrompt()\n",
    "    agent_prompt.add_context(dentaku_agent_template(\"1 + 1 =?\", \"1+1\"))\n",
    "    agent_prompt.add_context(dentaku_agent_template(\"2 x 2 =?\", \"2*2\"), True)\n",
    "    agent_prompt.add_context(dentaku_agent_template(\"4 / 2 =?\", \"4/2\"), True)\n",
    "    agent_prompt.add_context(dentaku_agent_template(\"3 - 2 =?\", \"3-2\"), True)\n",
    "\n",
    "    # 質問の入力\n",
    "    agent_prompt.add_speaker_context(\"USER\", f\"{d['質問']} {question}\", True)\n",
    "\n",
    "    # 考察と行動を導く\n",
    "    payload = {\n",
    "        \"inputs\": agent_prompt.contextual_prompt,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    response_text = query_endpoint(payload)\n",
    "    print(f\"{response_text}\")\n",
    "\n",
    "    return handler(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    answer = agent(\"3 + 5 =?\")\n",
    "    print(f\"回答: {answer}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算式以外の質問に対しては適切な Tool がないと回答されることが期待されます。期待通り動作する確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    answer = agent(\"あなたの名前は?\")\n",
    "    print(f\"回答: {answer}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "より高度に Agent を実装するなら\"完了\"を判断する仕組みを導入し、\"完了\"するまで質問、考察、行動を自動で繰り返す方法が考えられます。また、考察、行動により得られた結果を質問のプロンプトに含めることでコンテキストを考慮することができます。これらを実装する際には、[LangChain]( https://python.langchain.com/docs/get_started/introduction) の Agents モジュールを利用することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 発展\n",
    "- これらのプロンプトは一例です。より多くのユースケースがあります。どのようなユースケースに有用なモデルなのか、様々なプロンプトを与えて観察してみましょう。\n",
    "- 推論時のパラメータを変えて試してみましょう。生成停止条件やペナルティの与え方、確率によって回答が変わる幅などを変えることで回答がどのように変化するか観察してみましょう。\n",
    "- 推論 Endpoint の使い方に慣れることができたら、皆さんのシステムに組み込んでみましょう。もし、VPC や 暗号化が必要な場合は deploy 時に設定することができます。是非、挑戦してみましょう。\n",
    "- [aws-ml-jp](https://github.com/aws-samples/aws-ml-jp) は AWS で機械学習モデルを構築、学習、デプロイする方法が学べる Notebook と教材集です。生成系 AI に関するサンプルもあります。探索してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 付録: テキスト生成実行時に渡せるパラメータの説明\n",
    "[Hugging Face の Text Generation Inference](https://huggingface.co/blog/sagemaker-huggingface-llm) に則り、以下のパラメータをテキスト生成実行時に渡すことができます。この Notebook では、`max_new_tokens`, `repetition_penalty` などが該当します。\n",
    "\n",
    "- temperature: モデル内のランダム性を制御します。低い値はモデルをより決定論的にし、高い値はモデルをよりランダムにします。デフォルト値は1.0です。\n",
    "- max_new_tokens: 生成するトークンの最大数です。デフォルト値は20で、最大値は512です。\n",
    "- repetition_penalty: 繰り返しの発生確率を制御します。デフォルトはnullです。\n",
    "- seed: ランダム生成に使用するシードです。デフォルトはnullです。\n",
    "- stop: 生成を停止するトークンのリストです。これらのトークンのいずれかが生成されると、生成は停止します。\n",
    "- top_k: 最も高い確率の語彙トークンを保持する数を制御します。デフォルト値はnullで、トップ K フィルタリングを無効にします。\n",
    "- top_p: デフォルトはnullで、nucleus sampling のための最も高い確率の語彙トークンを保持するための累積確率を制御します。\n",
    "- do_sample：サンプリングを使用するかどうかです。デフォルト値はfalseです。それ以外の場合、サンプリングなしでデコードされます。\n",
    "- best_of: 最大 log 確率のトークンで生成した best_of シーケンスを返すかどうかです。デフォルトはnullです。\n",
    "- details: 生成に関する詳細情報を返すかどうかです。デフォルト値は false です。\n",
    "- return_full_text: 完全なテキストを返すかどうかです。デフォルト値は false です。それ以外の場合、生成された部分のみを返します。\n",
    "- truncate: モデルの最大長に切り詰めるかどうかです。デフォルト値は true です。\n",
    "- typical_p: トークンの典型的な確率です。デフォルトは null です。\n",
    "- watermark: 生成に使用するウォーターマークです。デフォルト値は false です。"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
