{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock と Amazon Kendra を用いた Advanced RAG 構築\n",
    "\n",
    "本ノートブックでは、Amazon Bedrock を生成器、Amazon Kendra を検索器として RAG システムを構築します。下記の順番で、まずは抽象度低く Naive RAG を構築し、動作を理解してから LangChain や LangGraph を用いた Advanced RAG システムの構築を体験します。\n",
    "\n",
    "1. AWS SDK for Python (boto3) を用いて Naive RAG を構築\n",
    "2. LangChain を用いて Naive RAG を構築\n",
    "3. LangGraph を用いて Naive RAG & Advanced RAG を構築\n",
    "\n",
    "## 1. 事前準備\n",
    "\n",
    "Ctrl + \\` で Code Editor (OSS 版 VS Code) のターミナルを開き、以下のコマンドを実行してください。\n",
    "\n",
    "```bash:\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -U pip\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. boto3 を直接用いて RAG を構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なモジュールのインポートと boto3 クライアントの準備\n",
    "\n",
    "AWS SDK for Python (boto3) 等のモジュールをインポートし、事前準備を行います。\n",
    "ノートブックの各セルを実行するには `Shift + Enter` を押してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 正規表現\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from IPython.display import (\n",
    "    display,\n",
    "    Markdown,\n",
    ")\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Amazon Kendra と Amazon Bedrock のクライアント\n",
    "kendra = boto3.client(\"kendra\", region_name=\"us-west-2\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "# Amazon Bedrock で利用する Claude 3 Haiku のモデル ID\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "accept = \"application/json\"\n",
    "content_type = \"application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon Kendra のインデックス ID を入力\n",
    "\n",
    "ワークショップの手順書のタイトルをクリックしてイベントのダッシュボードを開き、パラメーターのリスト (Event Outputs) にある `KendraIndexID` の値を以下にコピーしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 自身の環境に合わせて書き換える\n",
    "kendra_index_id = \"1e74d846-4929-49a4-83bb-00f4c97baf2a\"  # 36 文字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### まずは Amazon Bedrock に直接問いかけてみる\n",
    "\n",
    "`query_text` に好きな質問を入れて Claude 3 Haiku に問いかけてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユーザーの質問\n",
    "query_text = \"Amazon Bedrockのモデル評価機能ではどのようなことができますか？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon Bedrock に送るリクエストのボディー\n",
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,  # 最大出力トークン数\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "        # {\"role\": \"assistant\", \"content\": \"いい質問ですね。\"},\n",
    "        # のように user -> assistant -> user -> ... の会話形式にすることも可能\n",
    "    ],\n",
    "    # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",  # 固定\n",
    "    \"temperature\": 0,  # 出力のランダム度\n",
    "})\n",
    "\n",
    "try:\n",
    "    # Amazon Bedrock の InvokeModel API を実行\n",
    "    response_bedrock = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=model_id, accept=accept, contentType=content_type,\n",
    "    )\n",
    "    response_body = json.loads(response_bedrock.get(\"body\").read())\n",
    "    display(Markdown(response_body.get(\"content\")[0][\"text\"]))\n",
    "except ClientError as error:\n",
    "    raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InvokeModel API の理解を深める\n",
    "\n",
    "なお、`response_bedrock` の中身は以下のようになっており、`body` キーには `StreamingBody` の形で出力が格納されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`response_body` は JSON であり、`content` キーの中に辞書のリスト形式で出力文が格納されます。他にも例えば `usage` キーで入力と出力のトークン数を確認でき、実際にどの程度の料金がかかったかを計算することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Kendra で検索する\n",
    "\n",
    "AWS のインテリジェント検索サービスである Amazon Kendra の検索を試してみましょう。\n",
    "\n",
    "Kendra でドキュメントを検索するには [Query API](https://docs.aws.amazon.com/ja_jp/kendra/latest/APIReference/API_Query.html) と [Retrieve API](https://docs.aws.amazon.com/ja_jp/kendra/latest/APIReference/API_Retrieve.html) のふたつの方法があります。今回は、RAG との親和性が高い Retrieve API を用いることで、データソースの中のドキュメントから、ユーザーの質問内容に関連する抜粋部分を抽出し、生成 AI への入力としていきます。\n",
    "\n",
    "なお、Retrieve API では `PageSize` (デフォルト値は10件) を超える抜粋部分が抽出された際、レスポンスがページングされ、追加の `PageSize` 分ごとに結果を再取得していく必要があります。本ワークショップで RAG を実装する際には、最初の10件の抜粋のみを後続の処理に渡していくこととします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_kendra = kendra.retrieve(\n",
    "    QueryText=query_text,  # 検索クエリ\n",
    "    IndexId=kendra_index_id,  # Kendra Index ID\n",
    "    AttributeFilter={\n",
    "        \"EqualsTo\": {\n",
    "            \"Key\": \"_language_code\",\n",
    "            \"Value\": {\n",
    "                \"StringValue\": \"ja\",  # 日本語ドキュメントを検索 (default: en)\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    #PageNumber=1,\n",
    "    PageSize=3,  # 一度に返すドキュメント抜粋の数 (default: 10)\n",
    ")\n",
    "\n",
    "print(\"Query Text:\", query_text)\n",
    "print(\"Number of Retrieved Items:\", len(response_kendra[\"ResultItems\"]))\n",
    "\n",
    "for ii, item in enumerate(response_kendra[\"ResultItems\"]):\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Item\", ii + 1)\n",
    "    print(\"DocumentTitle:\", item[\"DocumentTitle\"])\n",
    "    print(\"Content\")\n",
    "    pprint(item[\"Content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve API の理解を深める\n",
    "\n",
    "Retrieve API の `response_kendra` の中にある `ResultItems` キーに、検索で得られたドキュメントの抜粋の情報が格納されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_kendra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ResultItems` の各項目の中身は [`RetrieveResultItem`](https://docs.aws.amazon.com/kendra/latest/APIReference/API_RetrieveResultItem.html) オブジェクトです。`RetrieveResulltItem` は以下の情報を含みます。\n",
    "\n",
    "- `Content`: 検索でヒットしたドキュメントの抜粋部分のテキスト\n",
    "- `DocumentAttributes`: ドキュメントのソース URI や抜粋のページ番号などの情報\n",
    "- `DocumentId`: ドキュメントの ID\n",
    "- `DocumentTitle`: ドキュメントのタイトル\n",
    "- `DocumentURI`: ドキュメントの URI\n",
    "- `Id`: ドキュメントの抜粋のユニークな ID\n",
    "- `ScoreAttribute`: クエリとの関連度合いのスコア (日本語は未対応で `NOT_AVAILABLE` が返る)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response_kendra[\"ResultItems\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、Amazon Kendra の Retrieve API で得られたドキュメントの抜粋を Amazon Bedrock に送るプロンプトに含めることで RAG を実現していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock と Kendra を組み合わせて RAG を実行する\n",
    "\n",
    "ここまでの情報だけで RAG を実現することができます。LLM に与えるプロンプトの中に Kendra で取得したドキュメントの抜粋を入れ込み、質問に答えてもらうように指示します。\n",
    "\n",
    "#### RAG 用のプロンプト\n",
    "\n",
    "サンプルのプロンプトテンプレートは以下の通りです。ここでは、`<excerpt>` (抜粋) タグ内にドキュメントの抜粋を記載し、`<query>` タグ内に質問文を記載しています。タグ名は固定ではなく、任意の値を設定できます。\n",
    "\n",
    "XML タグを理解するように学習されているのは Claude の特徴であり、XML タグを利用することで構造化されたプロンプトを簡単に構築することができます。詳しくは [Anthropic のプロンプトエンジニアリングガイド](https://docs.anthropic.com/ja/docs/use-xml-tags)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "あなたは親切なチャットアシスタントです。\n",
    "\n",
    "<excerpts>タグにはユーザーが知りたい情報を検索した結果のドキュメントの抜粋が含まれています。\n",
    "ドキュメントの抜粋は複数あり、それぞれの抜粋が<excerpt>タグで囲まれています。\n",
    "\n",
    "<excerpts>\n",
    "\"\"\"\n",
    "\n",
    "for item in response_kendra[\"ResultItems\"]:\n",
    "    prompt += f\"<excerpt>{item['Content']}</excerpt>\\n\"\n",
    "\n",
    "prompt += f\"\"\"</excerpts>\n",
    "\n",
    "<excerpts>タグ内の情報を参考にして、<query>タグ内のユーザーの質問に答えてください。\n",
    "\n",
    "<query>\n",
    "{query_text}\n",
    "</query>\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG の実行\n",
    "\n",
    "上記のプロンプトを利用して Amazon Bedrock の InvokeModel API を実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(prompt, model_id=\"anthropic.claude-3-haiku-20240307-v1:0\"):\n",
    "    body = json.dumps({\n",
    "        \"max_tokens\": 1024,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"temperature\": 0,\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response_bedrock = bedrock_runtime.invoke_model(\n",
    "            body=body, modelId=model_id, accept=accept, contentType=content_type,\n",
    "        )\n",
    "        response_body = json.loads(response_bedrock.get(\"body\").read())\n",
    "        return response_body.get(\"content\")[0][\"text\"]\n",
    "    except ClientError as error:\n",
    "        raise error\n",
    "\n",
    "print(invoke_model(prompt, model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### プロンプトエンジニアリング実践\n",
    "\n",
    "##### 1. 回答文に XML タグを含めないようにする\n",
    "\n",
    "`<excerpts>` や `<excerpt>` タグはシステム内部で用いるものであり、ユーザーからするとよくわからない謎の文字列です。\n",
    "このままではユーザーを不安にしてしまうので、回答生成に条件を追加し、回答に XML タグを含めないようにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "あなたは親切なチャットアシスタントです。\n",
    "\n",
    "<excerpts>タグにはユーザーが知りたい情報を検索した結果のドキュメントの抜粋が含まれています。\n",
    "ドキュメントの抜粋は複数あり、それぞれの抜粋が<excerpt>タグで囲まれています。\n",
    "\n",
    "<excerpts>\n",
    "\"\"\"\n",
    "\n",
    "for item in response_kendra[\"ResultItems\"]:\n",
    "    prompt += f\"<excerpt>{item['Content']}</excerpt>\\n\"\n",
    "\n",
    "prompt += f\"\"\"</excerpts>\n",
    "\n",
    "<excerpts>タグ内の情報を参考にして、<query>タグ内のユーザーの質問に答えてください。\n",
    "\n",
    "<query>\n",
    "{query_text}\n",
    "</query>\n",
    "\"\"\"\n",
    "\n",
    "rule = \"\"\"\n",
    "ただし、<rule>タグ内の回答ルールを遵守してください。\n",
    "\n",
    "<rule>\n",
    "- 回答には<excerpts>, <excerpt>, <query>タグを含めないこと。\n",
    "</rule>\n",
    "\"\"\"\n",
    "\n",
    "print(prompt + rule)\n",
    "print(\"-----\")\n",
    "print(invoke_model(prompt + rule, model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 検索インデックスに含まれていない内容は答えないようにする\n",
    "\n",
    "ここまでのプロンプトでは、Kendra に含まれていない知識に関する質問をされても LLM (Claude 3 Haiku) の持っている知識で答えようとします。\n",
    "\n",
    "<!--例えば、Amazon SageMaker Studio の上でネイティブに利用できる開発環境は、JupyterLab、RStudio、SageMaker Canvas、Code Editor (OSS 版 Visual Studio Code)、SageMaker Studio Classic なのですが、RAG システムに問い合わせると間違った答え (ハルシネーション) が返ってきます。-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"アマゾン川はどの国を通っていますか？\"\n",
    "\n",
    "# Kendra での検索 - 無関係なドキュメントが検索される\n",
    "response_kendra = kendra.retrieve(\n",
    "    QueryText=query_text,  # 検索クエリ\n",
    "    IndexId=kendra_index_id,  # Kendra Index ID\n",
    "    AttributeFilter={\n",
    "        \"EqualsTo\": {\n",
    "            \"Key\": \"_language_code\",\n",
    "            \"Value\": {\n",
    "                \"StringValue\": \"ja\",  # 日本語ドキュメントを検索 (default: en)\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    #PageNumber=1,\n",
    "    PageSize=3,  # 一度に返すドキュメント抜粋の数 (default: 10)\n",
    ")\n",
    "\n",
    "# LLM へのプロンプト\n",
    "prompt = \"\"\"\n",
    "あなたは親切なチャットアシスタントです。\n",
    "\n",
    "<excerpts>タグにはユーザーが知りたい情報を検索した結果のドキュメントの抜粋が含まれています。\n",
    "ドキュメントの抜粋は複数あり、それぞれの抜粋が<excerpt>タグで囲まれています。\n",
    "\n",
    "<excerpts>\n",
    "\"\"\"\n",
    "\n",
    "for item in response_kendra[\"ResultItems\"]:\n",
    "    prompt += f\"<excerpt>{item['Content']}</excerpt>\\n\"  # 質問と無関係なドキュメントを追加\n",
    "\n",
    "prompt += f\"\"\"</excerpts>\n",
    "\n",
    "<excerpts>タグ内の情報を参考にして、<query>タグ内のユーザーの質問に答えてください。\n",
    "\n",
    "<query>\n",
    "{query_text}\n",
    "</query>\n",
    "\"\"\"\n",
    "\n",
    "rule = \"\"\"\n",
    "ただし、<rule>タグ内の回答ルールを遵守してください。\n",
    "\n",
    "<rule>\n",
    "- 回答には<excerpts>, <excerpt>, <query>タグを含めないこと。\n",
    "</rule>\n",
    "\"\"\"\n",
    "\n",
    "print(prompt + rule)\n",
    "print(\"-----\")\n",
    "\n",
    "# RAG 実行\n",
    "print(invoke_model(prompt + rule, model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は、ナレッジベース (Kendra) から関連するドキュメントが検索できなかった場合には無理に回答しないようにプロンプトを工夫してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = \"\"\"\n",
    "ただし、<rule>タグ内の回答ルールを遵守してください。\n",
    "\n",
    "<rule>\n",
    "- 回答には<excerpts>, <excerpt>, <query>タグを含めないこと。\n",
    "- 回答は<answer>タグで囲むこと。\n",
    "- 回答する前に、質問へ回答するために必要な情報が与えられているか考え、<thinking>タグに判断の理由を書いてください。\n",
    "</rule>\n",
    "\"\"\"\n",
    "\n",
    "print(prompt + rule)\n",
    "print(\"-----\")\n",
    "\n",
    "# RAG 実行\n",
    "output = invoke_model(prompt + rule, model_id)\n",
    "print(output)\n",
    "print(\"-----\")\n",
    "\n",
    "match = re.search(r\"<answer>(.*?)</answer>\", output, re.DOTALL)\n",
    "if match:\n",
    "    answer = match.group(1).strip()\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この、最終的な答えを提供する前に Claude に考える時間を与える手法は「思考の連鎖 (Chain of Thoughts; CoT)」プロンプトと呼ばれます。\n",
    "Claude の段階的な推論と最終的な応答を区別しやすくするために、`<thinking>` や `<answer>` タグを使うことが推奨されています。\n",
    "詳しくは [Anthropic のプロンプトエンジニアリングガイド](https://docs.anthropic.com/ja/docs/let-claude-think#claude-2)を参照してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 回答の根拠となる引用を付ける\n",
    "\n",
    "本セクションの締めくくりとして、回答文だけでなく、回答の根拠となる文書を引用するようにしてみましょう。\n",
    "\n",
    "まず、プロンプトに Kendra で検索したドキュメントの抜粋だけでなく、ドキュメント ID やタイトルも含めるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Amazon Bedrockって何ですか？\"\n",
    "\n",
    "response_kendra = kendra.retrieve(\n",
    "    QueryText=query_text,  # 検索クエリ\n",
    "    IndexId=kendra_index_id,  # Kendra Index ID\n",
    "    AttributeFilter={\n",
    "        \"EqualsTo\": {\n",
    "            \"Key\": \"_language_code\",\n",
    "            \"Value\": {\n",
    "                \"StringValue\": \"ja\",  # 日本語ドキュメントを検索 (default: en)\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    #PageNumber=1,\n",
    "    PageSize=10,  # 一度に返すドキュメント抜粋の数 (default: 10)\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "あなたは親切なチャットアシスタントです。\n",
    "\n",
    "<excerpts>タグにはユーザーが知りたい情報を検索した結果のドキュメントの抜粋が含まれています。\n",
    "ドキュメントの抜粋は複数あり、それぞれの抜粋が<excerpt>タグで囲まれています。\n",
    "\n",
    "<excerpts>\n",
    "\"\"\"\n",
    "\n",
    "for item in response_kendra[\"ResultItems\"]:\n",
    "    prompt += \"<excerpt>\"\n",
    "    prompt += f\"<document_id>{item['DocumentId']}</document_id>\"\n",
    "    prompt += f\"<title>{item['DocumentTitle']}</title>\"\n",
    "    prompt += f\"<content>{item['Content']}</content>\"\n",
    "    prompt += \"</excerpt>\\n\"\n",
    "\n",
    "\n",
    "prompt += f\"\"\"</excerpts>\n",
    "\n",
    "<excerpts>タグ内の情報を参考にして、<query>タグ内のユーザーの質問に答えてください。\n",
    "\n",
    "<query>\n",
    "{query_text}\n",
    "</query>\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "追加したドキュメント ID やタイトルの情報をもとに、回答の根拠となるドキュメントを引用するようにします。ここでは、LLM の出力を XML タグで構造化し、それをレンダリングする際にパースする方針を取ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = \"\"\"\n",
    "回答内で検索結果からの情報を参照する場合は、情報が見つかったソースのドキュメントへの引用を含める必要があります。\n",
    "各結果には、参照すべき対応するdocument_idとtitleを付与します。\n",
    "回答に複数のドキュメントの抜粋からの情報が含まれている場合、<sources>には複数の<source>が含まれる可能性があることに注意してください。\n",
    "<excerpts>を回答で直接引用しないでください。あなたの仕事は、ユーザーの質問に可能な限り簡潔に答えることです。\n",
    "以下の形式で回答を出力する必要があります。フォーマットとスペースに注意して、正確に従ってください。\n",
    "<answer>\n",
    "  <answer_part>\n",
    "    <text>\n",
    "      最初の回答テキスト\n",
    "    </text>\n",
    "    <sources>\n",
    "      <source>\n",
    "        <document_id>document_id</document_id>\n",
    "        <title>title</title>\n",
    "      </source>\n",
    "    </sources>\n",
    "  </answer_part>\n",
    "  <answer_part>\n",
    "    <text>\n",
    "      2番目の回答テキスト\n",
    "    </text>\n",
    "    <sources>\n",
    "      <source>\n",
    "        <document_id>document_id</document_id>\n",
    "        <title>title</title>\n",
    "      </source>\n",
    "    </sources>\n",
    "  </answer_part>\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt + rule)\n",
    "print(\"-----\")\n",
    "\n",
    "# RAG 実行\n",
    "output = invoke_model(prompt + rule, model_id)\n",
    "print(output)\n",
    "print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規表現を使って XML タグをパースします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(xml):\n",
    "    # XMLデータから<answer_part>タグで囲まれた部分を抽出する\n",
    "    answer_parts = re.findall(r'<answer_part>(.*?)</answer_part>', xml, re.DOTALL)\n",
    "    result = \"\"  # 結果を格納する変数\n",
    "    source_refs = \"\"  # ソース情報を格納する変数\n",
    "    source_index = 1  # ソースに付与する連番\n",
    "    unique_refs = set()  # 重複するソースのタイトルを避けるための集合\n",
    "    source_dic = {}  # ソースのタイトルと番号のマッピング\n",
    "    for part in answer_parts:\n",
    "        # 回答の本文を抽出する\n",
    "        text = re.search(r'<text>(.*?)</text>', part, re.DOTALL).group(1).strip()\n",
    "        # ソース情報を抽出する\n",
    "        sources = re.findall(r'<source>(.*?)</source>', part, re.DOTALL)\n",
    "        source_str = \"\"  # 現在の部分のソース情報 (e.g. [1]) を格納する文字列\n",
    "        for source in sources:\n",
    "            document_id = re.search(r'<document_id>(.*?)</document_id>', source).group(1)\n",
    "            title = re.search(r'<title>(.*?)</title>', source).group(1)\n",
    "            # ソースのタイトルが重複する場合は、前に出現したものと同じ番号を付与する\n",
    "            if title in unique_refs:\n",
    "                source_str += f\" \\[{source_dic[title]}\\]\"\n",
    "            else:\n",
    "                source_str += f\" \\[{source_index}\\]\"\n",
    "                unique_refs.add(title)\n",
    "                source_dic[title] = source_index\n",
    "                source_refs += f\"\\[{source_index}\\] [{title}]({document_id})  \\n\"  # ソース情報を追加\n",
    "                source_index += 1\n",
    "        # 回答の本文とソース情報を結果に追加する\n",
    "        result += f\"{text}{source_str}\\n\\n\"\n",
    "    # 最後にソースの連番とタイトルを追加する\n",
    "    return result + \"\\n\\n\" + source_refs\n",
    "\n",
    "\n",
    "display(Markdown(parse_answer(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-retrieval: クエリ拡張\n",
    "\n",
    "クエリ拡張は、単一のクエリを複数のクエリに拡張することで多様な検索結果を取得し、生成される回答の適合性を高めるための手法です。クエリ拡張にも、単純に複数の異なるクエリを作成するマルチクエリ (multi query) や、元の質問を分解して個々の質問に答えるためのクエリを作成するサブクエリ (sub query) などさまざまなアプローチがありますが、今回はシンプルなマルチクエリのアプローチを採用しています。LLM は generator と同様に Claude 3 Haiku を用いてクエリを拡張します。\n",
    "\n",
    "必ずしもユーザーのクエリとソースドキュメントの表現が一致しているわけではありません。「ナレッジベース」と「Knowledge Bases」等、日本語と英語との表記揺れや、類義語、タイポなど、さまざまな要因で初期のクエリでは回答に必要な情報が十分に取得できないことがあります。クエリを拡張し、類似するキーワードを複数用いることで、キーワード検索とベクトル検索のハイブリッド検索のような、確実性と曖昧性を兼ね備えた検索が実現されることが期待されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries = 3\n",
    "\n",
    "prompt = f\"\"\"\n",
    "検索エンジンに入力するクエリを最適化し、様々な角度から検索を行うことで、より適切で幅広い検索結果が得られるようにします。 \n",
    "以下の<question>タグ内にはユーザーの入力した質問文が入ります。\n",
    "この質問文に基づいて、{n_queries}個の検索用クエリを生成してください。\n",
    "各クエリは30トークン以内とし、日本語と英語を適切に混ぜて使用することで、広範囲の文書が取得できるようにしてください。\n",
    "\n",
    "<question>\n",
    "{query_text}\n",
    "<question>\n",
    "\"\"\"\n",
    "\n",
    "print(invoke_model(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このままだとプログラム的に扱いづらいので出力フォーマットを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_format = \"\"\n",
    "for i in range(n_queries):\n",
    "    output_format += f\"query {i+1}: fill a query here\\n\"\n",
    "\n",
    "format = f\"\"\"\n",
    "生成されたクエリは、<format>タグ内のフォーマットに従って出力してください。\n",
    "\n",
    "<format>\n",
    "{output_format}\n",
    "</format>\n",
    "\"\"\"\n",
    "\n",
    "queries = invoke_model(prompt + output_format)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あとは正規表現で抜き出してあげれば後段の Kendra への入力にすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [query for query in queries.split(\"\\n\") if re.match(r'query \\d+:[^:]+$', query)]\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-retrieval: 検索結果の関連度評価\n",
    "\n",
    "このステップでは検索結果が、元のユーザーからの質問に関連したものになっているかを評価します。検索で得られたドキュメントの抜粋 (チャンク) というのは必ずしも質問に回答するための情報を含んでいるとは限らず、誤った回答を誘発するような内容も含んでしまっていることがあります。例えば、“[Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884)” \\[Shi-Qi Yan et al. (2024)\\] の論文で提案されている手法 (Corrective RAG; CRAG) では、検索結果の関連度を評価し、その評価結果をもとに最終回答を生成します。CRAG ではドキュメントの抜粋をそれぞれ Correct (正確) / Ambiguous (曖昧) / Incorrect (不正確) のカテゴリに分類しますが、今回は簡単のために関連しているか否かの yes / no の二値に分類します。LLM は他のステップと同様に Claude 3 Haiku を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Amazon Bedrock で Claude 3 Sonnet の基盤モデルに関する情報を取得する Python コードを教えて\"\n",
    "response_kendra = kendra.retrieve(\n",
    "    QueryText=query_text,  # 検索クエリ\n",
    "    IndexId=kendra_index_id,  # Kendra Index ID\n",
    "    AttributeFilter={\"EqualsTo\": {\"Key\": \"_language_code\", \"Value\": {\"StringValue\": \"ja\",}}},\n",
    "    PageSize=5,  # 一度に返すドキュメント抜粋の数 (default: 10)\n",
    ")\n",
    "\n",
    "result_items = []\n",
    "\n",
    "for ii, item in enumerate(response_kendra[\"ResultItems\"]):\n",
    "    result_item = \"\"\n",
    "    result_item += \"-\" * 30 + \"\\n\"\n",
    "    result_item += f\"Item {ii+1}\\n\"\n",
    "    result_item += f\"DocumentTitle:{item['DocumentTitle']}\\n\"\n",
    "    result_item += \"Content\\n\"\n",
    "    result_item += item[\"Content\"] + \"\\n\"\n",
    "    result_items.append(result_item)\n",
    "\n",
    "pprint(result_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_item in result_items:\n",
    "    prompt = f\"\"\"\n",
    "    あなたは、ユーザーからの質問と検索で得られたドキュメントの関連性を評価する専門家です。\n",
    "    <excerpt>タグ内は、検索により取得したドキュメントの抜粋です。\n",
    "\n",
    "    <excerpt>{result_item}</excerpt>\n",
    "    \n",
    "    <question>タグ内は、ユーザーからの質問です。\n",
    "\n",
    "    <question>{query_text}</question>\n",
    "    \n",
    "    このドキュメントの抜粋は、ユーザーの質問に回答するための正確な情報を含んでいるかを慎重に判断してください。\n",
    "    正確な情報を含んでいる場合は 'yes'、含んでいない場合は 'no' のバイナリスコアを返してください。\n",
    "    バイナリスコアは<related></related>タグ内に格納してください。\n",
    "    \"\"\"\n",
    "\n",
    "    print(invoke_model(prompt))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、Kendra の Retrieve API で得られたドキュメントの抜粋ひとつひとつに対して関連度評価を行っています。クエリ拡張と組み合わせた場合 (元のクエリ + 3つの拡張クエリ) は、4クエリ×10抜粋=40抜粋分、Claude 3 Haiku を呼び出すことになります。1抜粋当たり、プロンプト全体で800トークン前後になるため、一回の検索における関連度評価だけで30,000トークン程度消費するケースがあることになります。レイテンシーの影響も最小限に抑えるため、クエリ拡張と同様に非同期処理として実行するのが望ましいですが、運用の際には RPM (Requests processed per minute) や TPM (Tokens processed per minute) のクォータには注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG を LangChain で構築する\n",
    "\n",
    "LangChain は、大規模言語モデル (LLM) を活用したアプリケーション開発のためのオープンソースフレームワークです。様々な言語モデル、データソース、API を組み合わせて、コンテキストを理解し推論できるアプリケーションを構築できます。プロンプトテンプレート、メモリ管理、モジュール化されたアーキテクチャなどの機能を備えており、チャットボット、質問応答システム、要約ツールなど、多様なユースケースに適用可能です。\n",
    "\n",
    "langchain_aws は、LangChain と AWS を連携させる Python パッケージです。Amazon Bedrock、Amazon Kendra などのサービスと連携でき、AmazonKendraRetriever や ChatBedrock など AWS 固有のコンポーネントを提供しています。AmazonKendraRetriever は Amazon Kendra を利用したドキュメント検索、ChatBedrock は Amazon Bedrock の LLM を使ったチャットモデルです。\n",
    "\n",
    "langchain_aws を使えば、AWS の豊富な機能と LangChain の柔軟性を組み合わせ、実用的で拡張性の高い自然言語処理アプリケーションを構築できます。LLM の能力をさらに引き出し、外部データと統合させた高度なアプリケーション開発が可能になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なモジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import AmazonKendraRetriever\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "import langchain\n",
    "\n",
    "langchain.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AmazonKendraRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = AmazonKendraRetriever(\n",
    "    index_id=kendra_index_id,\n",
    "    region_name=\"us-west-2\",\n",
    "    attribute_filter={\"EqualsTo\": {\"Key\": \"_language_code\", \"Value\": {\"StringValue\": \"ja\"}}},\n",
    "    top_k=10,\n",
    ")\n",
    "retriever.invoke(input=query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    region_name=\"us-west-2\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 1024,\n",
    "    }\n",
    ")\n",
    "chat.invoke(input=query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "あなたは親切なチャットアシスタントです。\n",
    "\n",
    "<excerpts>タグにはユーザーが知りたい情報を検索した結果のドキュメントの抜粋が含まれています。\n",
    "ドキュメントの抜粋は複数あり、それぞれの抜粋が<excerpt>タグで囲まれています。\n",
    "\n",
    "<excerpts>\n",
    "{context}\n",
    "</excerpts>\n",
    "\n",
    "<excerpts>タグ内の情報を参考にして、ユーザーの質問に答えてください。\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain\n",
    "\n",
    "ここまでは boto3 を直接叩くのと対して変わらないですが、LamgChain はその名の通り各コンポーネントをチェーンのように繋ぐことができるのが便利なところです。単純な RAG システムを構築するためのモジュールが用意されており、以下のようにして全体のフローをひとつのチェーンとして実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chat, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "chain.invoke({\"input\": query_text})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参考\n",
    "LangChain において文書を要約したり質問に答えたりする際に、複数の文書をどのように処理するかはいくつか方式があります。主に以下の4つのタイプがあります。\n",
    "1. stuff\n",
    "- 全ての文書を1つのプロンプトに詰め込んで (stuff)、1回の API 呼び出しで処理する最もシンプルな方式。\n",
    "- 文書が小さく数が少ない場合に適している。\n",
    "- 大きな文書にはあまり向かない。\n",
    "2. map_reduce\n",
    "- 各文書に対して個別に LLM を適用し (map)、その結果を新しい文書とみなす。\n",
    "- それらの新しい文書を別の文書結合チェーンに渡して単一の出力を得る (reduce)。\n",
    "- 必要に応じて map された文書を再帰的に圧縮する。\n",
    "- 大きな文書を扱うのに適している。\n",
    "- API 呼び出し回数が多くなる。\n",
    "3. refine\n",
    "- 入力文書をループして反復的に回答を更新していく方式。\n",
    "- 各文書について、現在の中間回答と共に LLM に渡し、新しい回答を得る。\n",
    "- 1度に1つの文書しか LLM に渡さないため、多くの文書を分析するのに適している。\n",
    "- stuff よりもはるかに多くの LLM 呼び出しが必要。\n",
    "4. map_rerank\n",
    "- 各文書に対してプロンプトを実行し、タスクを完了させるだけでなく、回答の確からしさもスコア付けさせる。\n",
    "- スコアが最も高い回答を返す。\n",
    "以上のように、扱う文書の量や目的に応じて適切な chain type を選択することで、LangChain を使った文書の要約や質問応答をより効果的に行うことができます。\n",
    "\n",
    "https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/chains/combine_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph\n",
    "\n",
    "LangGraph は、LangChain をベースに構築された大規模言語モデルを用いたステートフルなマルチエージェントアプリケーションを開発するための Python ライブラリです。主な特徴は以下の通りです。\n",
    "\n",
    "1. サイクル(循環)の実装が容易で、LLM をループで呼び出しながら次のアクションを決定するようなフローを簡単に実装できる。\n",
    "2. グラフベースでエージェントのワークフローを定義し、グラフの状態を変更することで動的な応答が可能。\n",
    "3. アプリケーション全体の状態を管理する GraphState を定義でき、状態管理が容易。\n",
    "4. 複数の独立したエージェントを連携させて協調動作させることができる。\n",
    "5. 永続化機能により、長時間実行のマルチセッションアプリケーションに対応。\n",
    "6. LangChain エコシステムと統合されており、様々な機能を活用できる。\n",
    "\n",
    "他にも、Advanced RAG のような複雑なフローをグラフベースのアーキテクチャによって簡単に実装できることもメリットです。LangGraph を用いると高度な自然言語処理アプリケーションをモジュール形式で容易に実装できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば、以下のようなフローを作る際、Retrieve や Grade などのノードをエッジ (矢印) で繋ぐことで状態 (state) 遷移の流れを構築します。\n",
    "条件分岐も conditional edge と呼ばれるコンポーネントを利用することで簡単に実現できます。\n",
    "![langgraph_crag](img/crag.png)\n",
    "出典: [Corrective RAG (CRAG) - langchain-ai/langgraph](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--![langgraph_crag](img/crag.png)\n",
    "出典: [Corrective RAG (CRAG) - langchain-ai/langgraph](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb?ref=blog.langchain.dev)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各モジュールの構築\n",
    "\n",
    "今回構築するグラフ構造は下図の通りです。質問が投げられたら `entry_point` を経由して、ひとつ目の条件分岐に入ります。`n_queries` パラメータが1以上であれば `generate_queries` ノードで `n_queries` 個のクエリ拡張を行い、`retrieve` ノードで検索を行います。ふたつ目の条件分岐では、`grade_documents_enabled` パラメータが `\"Yes\"` であれば `grade_documents` ノードを実行し、フィルタされたドキュメントの抜粋を元に `generate` ノードで回答生成を行います。\n",
    "\n",
    "![graph](img/final_graph.png)\n",
    "\n",
    "#### State\n",
    "\n",
    "ノード間の状態 (state) は辞書形式で持つことにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entry Point\n",
    "\n",
    "はじめに条件分岐を置きたいため、グラフのエントリーポイントとしてはパススルーのノードをセットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry_point(state):\n",
    "    \"\"\"Pass through\"\"\"\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ひとつ目の条件分岐\n",
    "\n",
    "`n_queries` パラメータが1以上であれば `generate_queries_enable`、0以下であれば `generate_queries_not_enable` と判断するエッジのロジックを書きます。\n",
    "\n",
    "![decide_to_generate_queries](img/decide_to_generate_queries.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate_queries(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate queries, or use the original query.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE QUERIES---\")\n",
    "    n_queries = state[\"keys\"][\"n_queries\"]\n",
    "\n",
    "    if n_queries > 0:\n",
    "        print(\n",
    "            \"---DECISION: GENERATE QUERIES---\"\n",
    "        )\n",
    "        return \"generate_queries_enabled\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT GENERATE QUERIES---\")\n",
    "        return \"generate_queries_not_enabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Queries: クエリ拡張\n",
    "\n",
    "元のクエリ (`question`) に加えて `n_queries` のクエリを拡張するノードを定義します。\n",
    "\n",
    "![generate_queries](img/generate_queries.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(state):\n",
    "    \"\"\"Generate a variety of queries (RAG-Fusion).\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): The updated graph state with generated queries.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE QUERIES---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    n_queries = state_dict[\"n_queries\"]\n",
    "\n",
    "    llm = ChatBedrock(\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        region_name=\"us-west-2\",\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 512,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    output_format = \"\"\n",
    "    for i in range(n_queries):\n",
    "        output_format += f\"{i+1}: fill a query here\\n\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "検索エンジンに入力するクエリを最適化し、様々な角度から検索を行うことで、より適切で幅広い検索結果が得られるようにします。 \n",
    "具体的には、類義語や日本語と英語の表記揺れを考慮し、多角的な視点からクエリを生成します。\n",
    "\n",
    "以下の<question>タグ内にはユーザーの入力した質問文が入ります。\n",
    "この質問文に基づいて、{n_queries}個の検索用クエリを生成してください。\n",
    "各クエリは30トークン以内とし、日本語と英語を適切に混ぜて使用することで、広範囲の文書が取得できるようにしてください。\n",
    "\n",
    "生成されたクエリは、<format>タグ内のフォーマットに従って出力してください。\n",
    "\n",
    "<example>\n",
    "question: Knowledge Bases for Amazon Bedrock ではどのベクトルデータベースを使えますか？\n",
    "query 1: Knowledge Bases for Amazon Bedrock vector databases engine DB store\n",
    "query 2: Amazon Bedrock ナレッジベース ベクトル ベクター エンジン データベース ストア インデックス\n",
    "query 3: Amazon Bedrock RAG 検索拡張生成 埋め込み embedding ベクトル データベース エンジン\n",
    "</example>\n",
    "\n",
    "<format>\n",
    "{output_format}\n",
    "</format> \n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\".format(\n",
    "        output_format=output_format,\n",
    "        n_queries=n_queries,\n",
    "        question=\"{question}\",  # as-is\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "    queries = chain.invoke({\"question\": question})\n",
    "    queries = [query.replace('\"', '') for query in queries]\n",
    "    queries = [query for query in queries if re.match(r'^\\d+:[^:]+$', query)]  # 数字:文字列の項目のみ抽出\n",
    "    queries = [f\"0: {question}\"] + queries\n",
    "    print(queries)\n",
    "\n",
    "    state_dict[\"queries\"] = queries\n",
    "\n",
    "    return {\"keys\": state_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve: Amazon Kendra を用いた検索\n",
    "\n",
    "Amazon Kendra でクエリに基づいて検索するノードを定義します。\n",
    "\n",
    "![retrieve](img/retrieve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    retriever = AmazonKendraRetriever(\n",
    "        index_id=kendra_index_id,\n",
    "        region_name=\"us-west-2\",\n",
    "        attribute_filter={\"EqualsTo\": {\"Key\": \"_language_code\", \"Value\": {\"StringValue\": \"ja\"}}},\n",
    "        top_k=10\n",
    "    )\n",
    "    documents = []\n",
    "    if \"queries\" in state_dict:\n",
    "        queries = state_dict[\"queries\"]\n",
    "        for query in queries:\n",
    "            documents.extend(retriever.invoke(query))\n",
    "    else:\n",
    "        documents = retriever.invoke(question)\n",
    "    state_dict[\"documents\"] = documents\n",
    "    return {\"keys\": state_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ふたつ目の条件分岐\n",
    "\n",
    "`grade_documents_enabled` パラメータが `\"Yes\"` であれば `grade_documents_enabled` を返し、`\"No\"` であれば `grade_documents_not_enabled` を返すロジックを書きます。\n",
    "\n",
    "![decide_to_grade_documents](img/decide_to_grade_documents.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether to grade documents or not.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GRADE DOCUMENTS---\")\n",
    "    grade_documents_enabled = state[\"keys\"][\"grade_documents_enabled\"]\n",
    "\n",
    "    if grade_documents_enabled == \"Yes\":\n",
    "        print(\n",
    "            \"---DECISION: GRADE DOCUMENTS---\"\n",
    "        )\n",
    "        return \"grade_documents_enabled\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT GRADE DOCUMENTS---\")\n",
    "        return \"grade_documents_not_enabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grade Documents: 検索結果の関連度評価\n",
    "\n",
    "Amazon Kendra の検索結果が元の質問 (`question`) に関連したものになっているかを判断し、関連しないようであれば除外するノードを定義する。\n",
    "\n",
    "![grade_documents](img/grade_documents.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    llm = ChatBedrock(\n",
    "        model_id=model_id,\n",
    "        region_name=\"us-west-2\",\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 128,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    system_template = \"\"\"\n",
    "あなたはユーザーからの質問と取得したドキュメントの関連性を評価します。\n",
    "\n",
    "<excerpt>タグ内は、検索により取得したドキュメントの抜粋です。\n",
    "\n",
    "<excerpt>\n",
    "{context}\n",
    "</excerpt>\n",
    "\n",
    "ドキュメントの抜粋が、ユーザーの質問に関連するものであるかを判別してください。\n",
    "関連していれば 'yes'、していなければ 'no' の binary score を返してください。\n",
    "binary score は<related>タグに格納し、出力は <related>yes</related> もしくは <related>no</related> のみとしてください。\n",
    "\"\"\"\n",
    "\n",
    "    #parser = XMLOutputParser(tags=[\"related\"])\n",
    "    #print(parser.get_format_instructions())\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    filtered_docs = []\n",
    "\n",
    "    for doc in documents:\n",
    "        output = chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"context\": doc.page_content,\n",
    "                #\"format_instructions\": parser.get_format_instructions(),\n",
    "            }\n",
    "        )\n",
    "        pattern = r\"<related>(.*?)</related>\"\n",
    "        match = re.search(pattern, output)\n",
    "\n",
    "        if match:\n",
    "            score = match.group(1)\n",
    "            if score == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(doc)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "                continue\n",
    "        else:\n",
    "            print(\"マッチする文字列がありません。\")\n",
    "\n",
    "    search = \"Yes\" if len(filtered_docs) == 0 else \"No\"\n",
    "\n",
    "    state_dict[\"documents\"] = filtered_docs\n",
    "    state_dict[\"run_web_search\"] = search\n",
    "\n",
    "    return {\n",
    "        \"keys\": state_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate\n",
    "\n",
    "検索で得られたドキュメントの抜粋をもとに質問への回答生成を行うノードを定義する。\n",
    "\n",
    "![generate](img/generate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"Generate documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    system_template = \"\"\"\n",
    "あなたは親切なチャットアシスタントです。\n",
    "\n",
    "<excerpts>タグにはユーザーが知りたい情報を検索した結果のドキュメントの抜粋が含まれています。\n",
    "ドキュメントの抜粋は複数あり、それぞれの抜粋が<excerpt>タグで囲まれています。\n",
    "\n",
    "<excerpts>\n",
    "{context}\n",
    "</excerpts>\n",
    "\n",
    "<excerpts>タグ内の情報を参考にして、ユーザーの質問に答えてください。\n",
    "ただし、<rule>タグ内の回答ルールを遵守してください。\n",
    "\n",
    "<rule>\n",
    "- 回答には<excerpts>, <excerpt>, <query>タグを含めないこと。\n",
    "- 回答は<answer>タグで囲むこと。\n",
    "- 回答する前に、質問へ回答するために必要な情報が与えられているか考え、<thinking>タグに判断の理由を書いてください。\n",
    "</rule>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_template),\n",
    "            (\"human\", \"{question}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = ChatBedrock(\n",
    "        model_id=model_id,\n",
    "        region_name=\"us-west-2\",\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 1024,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    output = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", output, re.DOTALL)\n",
    "    if match:\n",
    "        generation = match.group(1).strip()\n",
    "    else:\n",
    "        generation = \"\"\n",
    "\n",
    "    state_dict[\"generation\"] = generation\n",
    "    return {\n",
    "        \"keys\": state_dict\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph 全体を構築\n",
    "\n",
    "![final_graph](img/final_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ノードを追加\n",
    "workflow.add_node(\"entry_point\", entry_point)\n",
    "workflow.add_node(\"generate_queries\", generate_queries)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# グラフのフローを定義\n",
    "workflow.set_entry_point(\"entry_point\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"entry_point\",\n",
    "    decide_to_generate_queries,\n",
    "    {\n",
    "        \"generate_queries_enabled\": \"generate_queries\",\n",
    "        \"generate_queries_not_enabled\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate_queries\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    decide_to_grade_documents,\n",
    "    {\n",
    "        \"grade_documents_enabled\": \"grade_documents\",\n",
    "        \"grade_documents_not_enabled\": \"generate\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"grade_documents\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# グラフのコンパイル\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph を実行\n",
    "\n",
    "LangGraph で構築したグラフは `app.stream(inputs)` で実行できます。for loop を用いることでグラフ構造に従ってそれぞれのノードが実行されていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_graph(app, inputs):\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            pprint(f\"Node '{key}':\")\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "    pprint(value[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下で `question` や `n_queries`、`grade_documents_enabled` の値を変えることで Advanced RAG の手法をオンオフしながら質問してみましょう。\n",
    "\n",
    "<b> ＊ 今回の実装では負荷の軽減のため Kendra の検索や関連度評価は同期的に実行していますが、非同期的に並列で実行することでレイテンシーを大幅に削減することができます。 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 質問集\n",
    "question = \"Knowledge Bases for Amazon Bedrock ではどういったベクトルデータベースを利用できますか？\"\n",
    "#question = \"Amazon Kendraを使ってWebサイトのコンテンツを検索可能にしたいと考えています。クロールの対象とするURLを制限する方法はありますか?\"\n",
    "#question = \"Kendra で使用できるデータソースを全部教えて\"\n",
    "#question = \"Amazon Kendra がサポートしているユーザーアクセス制御の方法は？\"\n",
    "#question = \"Amazon Kendra の検索分析のメトリクスには何がありますか？\"\n",
    "#question = \"Amazon Bedrock で Claude 3 Sonnet の基盤モデルに関する情報を取得する Python コードを教えて\"\n",
    "#question = \"ナレッジベースでのembeddingモデルの選択肢は？\"\n",
    "#question = \"Amazon Kendraで検索結果のランキングロジックをカスタマイズできますか？\"\n",
    "#question = \"Amazon Kendraにはどんなエディションがありますか？\"\n",
    "#question = \"Amazon Bedrock でモデルにアクセスするには何が必要ですか？\"\n",
    "#question = \"Bedrockのagent機能は東京リージョンでは使えますか？\"\n",
    "#question = \"外国語学習を効果的に進めるコツとは？\"\n",
    "#question = \"Amazon CloudFrontの仕組みについて説明してください。\"\n",
    "#question = \"Amazon EC2の特徴は何ですか？\"\n",
    "\n",
    "inputs = {\"keys\": {\n",
    "    \"question\": question, \n",
    "    \"n_queries\": -1,  # generate_queries_not_enabled\n",
    "    \"grade_documents_enabled\": \"No\",\n",
    "    #\"n_queries\": 3,  # generate_queries_enabled\n",
    "    #\"grade_documents_enabled\": \"Yes\",\n",
    "}}\n",
    "\n",
    "invoke_graph(app, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "本ノートブックでは、Amazon Bedrock と Amazon Kendra を用いて Naive RAG から Advanced RAG までを段階的に構築する方法について学びました。\n",
    "\n",
    "今回の Naive RAG の構成は、Amazon Kendra でドキュメントを検索し、その検索結果を Amazon Bedrock の Claude モデルに渡して回答を生成するシンプルなアプローチです。\n",
    "boto3 を直接使う方法と、LangChain を使ってコンポーネント化する方法を紹介しました。\n",
    "\n",
    "Advanced RAGでは、いくつかの手法を組み合わせることで、より高度な検索と回答生成を実現しました。\n",
    "\n",
    "- Pre-retrieval のクエリ拡張により、元のクエリだけでなく関連する複数のクエリで検索することで、より多様で適切な検索結果を得られるようにしました。\n",
    "- Post-retrieval の関連度評価では、検索結果の各ドキュメントが元の質問に関連しているかを判定し、無関係なドキュメントを除外することで、回答の精度向上を図りました。\n",
    "\n",
    "最後に、LangGraph というグラフベースのフレームワークを使って、Advanced RAG の一連の処理フローをモジュール化し、パラメータを切り替えるだけで簡単に実行できるようにしました。\n",
    "\n",
    "本ノートブックで紹介したテクニックを応用することで、LLM とナレッジベース、検索エンジンを組み合わせた高度な質問応答システムを構築することができます。実務への適用の際は、非同期処理の導入などパフォーマンスチューニングも必要になるでしょう。\n",
    "\n",
    "LLM の性能は目覚ましい発展を遂げていますが、すべての知識を言語モデル内に詰め込むのは現実的ではありません。社内の情報資産を安全に利活用しながら、言語モデルの汎用的な能力と組み合わせる本アプローチは、企業の生産性を高め、イノベーションを加速する有望な手段になると考えられます。ぜひ参考にしていただければ幸いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
