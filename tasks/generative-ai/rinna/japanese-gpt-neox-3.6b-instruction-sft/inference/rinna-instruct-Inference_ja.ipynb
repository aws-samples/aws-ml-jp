{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c348b8",
   "metadata": {},
   "source": [
    "# japanese-gpt-neox-3.6b-instruction-sft を SageMaker で Hosting\n",
    "このノートブックは、rinna の japanese-gpt-neox-3.6b-instruction-sft モデルを、ローカルで推論し、それを SageMaker Inference に移植するノートブックです。  \n",
    "モデルの詳細については [Hugging Face apanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft) を参照ください。 一度ローカルで推論する都合上、ml.g5.2xlarge インスタンスを使用します。\n",
    "SageMaker Notebooks の conda_pytorch_p39 カーネルと、SageMaker Studio Notebook の PyTorch 1.13 Python 3.9 GPU Optimized カーネルで動いた実績があります。  \n",
    "ノートブックは外部ファイルを参照していないので、どのディレクトリに配置してあっても動作します。  \n",
    "\n",
    "\n",
    "また、ノートブックを動かすにあたって、各セルを実行すれば動きますが、どのように動くかなどについては、[AI/ML DarkPark](https://www.youtube.com/playlist?list=PLAOq15s3RbuL32mYUphPDoeWKUiEUhcug) の特に [Amazon SageMaker 推論 Part2すぐにプロダクション利用できる！モデルをデプロイして推論する方法 【ML-Dark-04】【AWS Black Belt】](https://youtu.be/sngNd79GpmE) をご参照ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b950e17",
   "metadata": {},
   "source": [
    "## ローカル推論\n",
    "SageMaker 推論エンドポイントにホスティングする前に、このNotebook上で動作確認を行う\n",
    "### ローカルで動かすためのライブラリをインストール\n",
    "必要なモジュールをインストールする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0436c68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers einops SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d9a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fe9a8",
   "metadata": {},
   "source": [
    "### モデルのダウンロード\n",
    "tokenizer と model をダウンロードします。[How to use the model](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft#how-to-use-the-model) に沿って実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab583c32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"rinna/japanese-gpt-neox-3.6b-instruction-sft\", use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a11760b-b608-4934-980a-a9f142864071",
   "metadata": {
    "tags": []
   },
   "source": [
    "以下のセルはモデルを DL して読み込むため 3 分ほど時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4a1bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"rinna/japanese-gpt-neox-3.6b-instruction-sft\", \n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbf366",
   "metadata": {},
   "source": [
    "### モデルの保存\n",
    "ローカルで推論する前に、モデルをストレージに出力して、再度読み込みます。  \n",
    "SageMaker で Hosting する際はモデルをファイルから読み込むことが一般的で、ローカルで動かすときもその方法に則って行うと、SageMaker に移植しやすいためにこの手順を入れています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916ce42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf './model'\n",
    "!mkdir -p './model/code'\n",
    "model_dir = './model'\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57748f-63d7-421d-a84e-763954dff3c3",
   "metadata": {},
   "source": [
    "メモリを解放します(OOM 対策)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce465780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3acaf-eb12-485c-ba96-0092e6c9efa9",
   "metadata": {},
   "source": [
    "### モデルの再ロード\n",
    "ファイルから tokenizer と model をロードします。  \n",
    "model は 7 分程度かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c53b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363b71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab1e54-199a-4852-b208-a4f0cb0bdd75",
   "metadata": {},
   "source": [
    "### 推論する\n",
    "prompt の形式は [japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft#japanese-gpt-neox-3.6b-instruction-sft) にあるとおり、以下にすると良い結果を得られやすいです。\n",
    "* プロンプトはユーザーとシステムの会話形式で与える\n",
    "* 各発言は、以下形式に則る  \n",
    "    `{ユーザー, システム} : {発言}`\n",
    "* プロンプトの末尾は`システム:` で終了させる\n",
    "* 改行は`<NL>`を利用し、発言はすべて `<NL>` で区切る必要がある\n",
    "\n",
    "以下はプロンプトの例です。`<NL>`の埋め込みが大変なので、改行で書いて後で置換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c1a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''ユーザー: 世界自然遺産を列挙してください。\n",
    "システム: 膨大な数です。例えば国で絞ってください。\n",
    "ユーザー: イギリスでお願いします。\n",
    "システム:'''.replace('\\n','<NL>')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28fdbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac5909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        do_sample=True,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.01,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "output = output.replace(\"<NL>\", \"\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af127eb0-3b58-4bfe-9924-e1a96ccda420",
   "metadata": {},
   "source": [
    "無事動いたらローカルでやっていたことを SageMaker の Hosting を使って再現します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67269333",
   "metadata": {},
   "source": [
    "## SageMaker による推論\n",
    "\n",
    "### モジュールのロードと定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d30e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sm = boto3.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed6305",
   "metadata": {},
   "source": [
    "### 推論コードの作成\n",
    "先程実行したコードをもとに記述していきます。  \n",
    "まずは必要なモジュールを記述した requirements.txt を用意します。  \n",
    "今回は [deep-learning-containers](https://github.com/aws/deep-learning-containers)の HuggingFace のコンテナを使います。  \n",
    "einops と SentencePiece が不足しているので requirements.txt に記載します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fdc6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model/code/requirements.txt\n",
    "einops\n",
    "SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298edca-47af-4129-974f-8269574a46ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "先程実行したコードを SageMaker Inference 向けに改変します。\n",
    "1. `model_fn` でモデルを読み込みます。先程は huggingface のモデルを直接ロードしましたが、`model_dir` に展開されたモデルを読み込みます。\n",
    "2. `input_fn` で前処理を行います。\n",
    "    * json 形式のみを受け付け他の形式は弾くようにします。\n",
    "    * json 文字列を dict 形式に変換して `return` します。\n",
    "3. `predict_fn` で推論します。`temperature` などのパラメータも合わせて入力します。\n",
    "4. `output_fn` で json 形式にして `return` します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc4cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model/code/inference.py\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir).to(DEVICE)\n",
    "    return {'tokenizer':tokenizer,'model':model}\n",
    "\n",
    "def input_fn(data, content_type):\n",
    "    if content_type == 'application/json':\n",
    "        data = json.loads(data)\n",
    "    else:\n",
    "        raise TypeError('content_type is only allowed application/json')\n",
    "    return data\n",
    "\n",
    "def predict_fn(data, model):\n",
    "    prompt = data['prompt']\n",
    "    token_ids = model['tokenizer'].encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    do_sample = data['do_sample']\n",
    "    max_new_tokens = data['max_new_tokens']\n",
    "    temperature = data['temperature']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model['model'].generate(\n",
    "            token_ids.to(DEVICE),\n",
    "            do_sample=True,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.01,\n",
    "            pad_token_id=model['tokenizer'].pad_token_id,\n",
    "            bos_token_id=model['tokenizer'].bos_token_id,\n",
    "            eos_token_id=model['tokenizer'].eos_token_id\n",
    "        )\n",
    "    output = model['tokenizer'].decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "    output = output.replace(\"<NL>\", \"\\n\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "def output_fn(data, accept_type):\n",
    "    if accept_type == 'application/json':\n",
    "        data = json.dumps({'result' : data})\n",
    "    else:\n",
    "        raise TypeError('content_type is only allowed application/json')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1a2d0",
   "metadata": {},
   "source": [
    "### モデルアーティファクトの作成と S3 アップロード\n",
    "アーティファクト(推論コード + モデル)を tar.gz に固めます。時間がかかるので `pigz` で並列処理を行います。  \n",
    "ml.g5.2xlarge で 9 分ほどかかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12399d66-1427-42cf-a2a3-fe4390d94a32",
   "metadata": {},
   "source": [
    "※ SageMaker Studio を使っている場合は pigz が入っていないので、以下セルのコメントを解除してインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f47ea-cea6-4e44-9806-0fd85527c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt update -y\n",
    "# !apt install pigz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003b0fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!rm model.tar.gz\n",
    "%cd model/\n",
    "!tar  cv ./ | pigz -p 8 > ../model.tar.gz # 8 並列でアーカイブ\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9a3a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_s3_uri = sagemaker.session.Session().upload_data(\n",
    "    'model.tar.gz',\n",
    "    key_prefix='japanese-gpt-neox-3.6b-instruction-sft'\n",
    ")\n",
    "print(model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d0ab5",
   "metadata": {},
   "source": [
    "### SageMaker SDK を用いてデプロイ\n",
    "使用している API の詳細は以下を確認してください。  \n",
    "[Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7866334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "region = boto3.session.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe392f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 名前の設定\n",
    "model_name = 'japanese-gpt-neox-3-6b-instruction-sft'\n",
    "endpoint_config_name = model_name + 'Config'\n",
    "endpoint_name = model_name + 'Endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e5c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='huggingface',\n",
    "    region=region,\n",
    "    version='4.26',\n",
    "    image_scope='inference',\n",
    "    base_framework_version='pytorch1.13',\n",
    "    instance_type = 'ml.g5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d03094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data = model_s3_uri,\n",
    "    role = role,\n",
    "    image_uri = image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98cf65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fccf8b",
   "metadata": {},
   "source": [
    "### SageMaker SDK で推論\n",
    "model_fn の実行に時間がかかってしまい、エンドポイントが IN_SERVICE になっても、初回推論はしばらく動かないことがあります。  \n",
    "CloudWatch Logs に以下のような表示がある場合はしばらく待てば使えるようになります。  \n",
    "`[WARN] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.`  \n",
    "だいたい 6 分くらいかかるため、リトライを入れています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1077753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt 確認\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e3b3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "request = {\n",
    "    'prompt' : prompt,\n",
    "    'max_new_tokens' : 128,\n",
    "    'do_sample' : True,\n",
    "    'temperature' : 0.01,\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        output = predictor.predict(request)['result']\n",
    "        break\n",
    "    except:\n",
    "        sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e58d0-29d0-4ef5-bea5-920ddaf909c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c17da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896098a",
   "metadata": {},
   "source": [
    "## boto3 でデプロイと推論\n",
    "標準だと SageMaker SDK が入っていない環境からデプロイや推論する場合(例:AWS Lambda など)は、boto3 でデプロイや推論することも多いです。  \n",
    "以下のセルは boto3 で実行する方法を記述しています。\n",
    "各 API の詳細は Document を確認してください。  \n",
    "[SageMaker](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html)  \n",
    "[SageMakerRuntime](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec9d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "sm = boto3.client('sagemaker')\n",
    "smr = boto3.client('sagemaker-runtime')\n",
    "endpoint_inservice_waiter = sm.get_waiter('endpoint_in_service')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99785a65-0ce9-4970-a0d6-68869574829a",
   "metadata": {},
   "source": [
    "モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754ae19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': image_uri,\n",
    "        'ModelDataUrl': model_s3_uri,\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
    "            'SAGEMAKER_REGION': region,\n",
    "        }\n",
    "    },\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6c941-1147-490d-92cd-f550fcc346df",
   "metadata": {},
   "source": [
    "エンドポイントコンフィグの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118dc38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTrafic',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.g5.2xlarge',\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16921ff6-e09f-4efb-8955-8ae5ae209218",
   "metadata": {},
   "source": [
    "エンドポイントの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39686d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a4a1e-7a71-40e2-8adf-2c6a5f3d602c",
   "metadata": {},
   "source": [
    "エンドポイント作成の完了を待つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7b8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_inservice_waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={'Delay': 5,}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bbc72-5237-4763-b1a4-ed57b49ba5a3",
   "metadata": {},
   "source": [
    "推論を行います。  \n",
    "ただし初回推論時のみモデルのロードに 7 分ほどかかるため、先程同様リトライを入れています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91cb45-1e52-43a9-9f2f-d8067b909320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 確認\n",
    "print(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69159367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 推論\n",
    "smr = boto3.client('sagemaker-runtime')\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        response = smr.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Accept='application/json',\n",
    "            Body=json.dumps(request)\n",
    "        )\n",
    "        break\n",
    "    except:\n",
    "        sleep(60)\n",
    "output = json.loads(response['Body'].read().decode('utf-8'))['result']\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4042166-342b-4067-9292-1546302c17c6",
   "metadata": {},
   "source": [
    "お片付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b036b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f107a-34c9-43bf-863e-f2ff993a2d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-gpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
